{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inspect\n",
    "\n",
    "> Collection of functions that enable the visualization and inspection of obtained results in just a few lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path, PosixPath\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions:\n",
    "\n",
    "Before starting with the example usage, let´s quickly define two helper functions that we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_only_matching_xlsx_files(dir_path: Path, paradigm_id: str, week_id: Optional[int]=None) -> List[Path]:\n",
    "    filtered_filepaths = []\n",
    "    for filepath in dir_path.iterdir():\n",
    "        if filepath.name.endswith('.xlsx') and (paradigm_id in filepath.name):\n",
    "            if week_id != None:\n",
    "                if f'week-{week_id}.' in filepath.name:\n",
    "                    filtered_filepaths.append(filepath)\n",
    "            else:\n",
    "                filtered_filepaths.append(filepath)\n",
    "    return filtered_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_metadata_from_filename(filepath_session_results: Path, group_assignment_filepath: Path) -> Dict:\n",
    "    metadata = {}\n",
    "    metadata['line_id'], mouse_id, metadata['paradigm_id'], week_string_with_file_extension = filepath_session_results.name.split('_')\n",
    "    metadata['subject_id'] = f'{metadata[\"line_id\"]}_{mouse_id}'\n",
    "    metadata['week_id'] = week_string_with_file_extension[week_string_with_file_extension.find('-') + 1:week_string_with_file_extension.find('.')]\n",
    "    df_group_assignment = pd.read_excel(group_assignment_filepath)\n",
    "    if metadata['subject_id'] in df_group_assignment['subject_id'].unique():\n",
    "        metadata['group_id'] = df_group_assignment.loc[df_group_assignment['subject_id'] == metadata['subject_id'], 'group_id'].iloc[0]\n",
    "    elif metadata['subject_id'] in df_group_assignment['alternative_subject_id'].unique():\n",
    "        metadata['group_id'] = df_group_assignment.loc[df_group_assignment['alternative_subject_id'] == metadata['subject_id'], 'group_id'].iloc[0]    \n",
    "    else:\n",
    "        metadata['group_id'] = 'unknown'\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load all data:\n",
    "\n",
    "This obviously assumes that you were using the `gait_analysis` package to analyze your 2D tracking data created the corresponding result exports. To get some quick insights into your data, feel free to use the following collection of functions.\n",
    "\n",
    "First, you need to provide the filepath to the exported Excel files as `root_dir_path` as a Path object (see exmaple below). You can also specify if you´d like to right away filter only for a certain set of weeks or paradigms (you can also just pass a list with a single value for each). Please note that you have to provide the exact name of the respective Excel Tab you are interested in as:  `sheet_name`.\n",
    "\n",
    "Note: The group_assignment Excel Sheet is still quite customized & should be replaced by a more generalizable configs file (e.g. .yaml) in future versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def collect_all_available_data(root_dir_path: Path, # Filepath to the directory that contains the exported results .xlsx files\n",
    "                               group_assignment_filepath: Path, # Filepath to the group_assignments.xlsx file\n",
    "                               paradigm_ids: List[str], # List of paradigms of which the results shall be loaded\n",
    "                               week_ids: List[str],  # List of weeks from which the results shall be loaded\n",
    "                               sheet_name: str # Tab name of the exported results sheet to load, e.g. \"session_overview\"\n",
    "                              ) -> pd.DataFrame:\n",
    "    all_recording_results_dfs = []\n",
    "    for week_id in week_ids:\n",
    "        for paradigm_id in paradigm_ids:\n",
    "            tmp_matching_filepaths = get_only_matching_xlsx_files(dir_path = root_dir_path, paradigm_id = paradigm_id, week_id = week_id)\n",
    "            for filepath in tmp_matching_filepaths:\n",
    "                metadata = get_metadata_from_filename(filepath_session_results = filepath, group_assignment_filepath = group_assignment_filepath)\n",
    "                tmp_xlsx = pd.ExcelFile(filepath)\n",
    "                tmp_df = pd.read_excel(tmp_xlsx, sheet_name = sheet_name, index_col = 0)\n",
    "                for key, value in metadata.items():\n",
    "                    tmp_df[key] = value\n",
    "                all_recording_results_dfs.append(tmp_df)\n",
    "    df = pd.concat(all_recording_results_dfs)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if you´d like to inspect the overall session overview, use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "df = collect_all_available_data(root_dir_path = Path('/path/to/your/directory/with/the/exported/retults/'),\n",
    "                                group_assignment_filepath = Path('/filepath/to/your/group_assignment.xlsx'),\n",
    "                                paradigm_ids = ['paradigm_a', 'paradigm_b', 'paradigm_c'],\n",
    "                                week_ids = [1, 4, 8, 12, 14],\n",
    "                                sheet_name = 'session_overview')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Filter your data (optional):\n",
    "\n",
    "You might want to filter your data to only inspect a specific proportion of it. You can do so quite easily by using the `filter_dataframe()` funtion (see example usage below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def filter_dataframe(df: pd.DataFrame, filter_criteria: List[Tuple]) -> pd.DataFrame:\n",
    "    # assert all list have equal lenghts\n",
    "    valid_idxs_per_criterion = []\n",
    "    for column_name, comparison_method, reference_value in filter_criteria:\n",
    "        # assert valid key in comparison methods\n",
    "        # assert column name exists\n",
    "        if comparison_method == 'greater':\n",
    "            valid_idxs_per_criterion.append(df.loc[df[column_name] > reference_value].index.values)\n",
    "        elif comparison_method == 'smaller':\n",
    "            valid_idxs_per_criterion.append(df.loc[df[column_name] < reference_value].index.values)\n",
    "        elif comparison_method == 'equal_to':\n",
    "            valid_idxs_per_criterion.append(df.loc[df[column_name] == reference_value].index.values)\n",
    "        elif comparison_method == 'is_in_list':\n",
    "            valid_idxs_per_criterion.append(df.loc[df[column_name].isin(reference_value)].index.values)\n",
    "        elif comparison_method == 'is_nan':\n",
    "            valid_idxs_per_criterion.append(df.loc[df[column_name].isnull()].index.values)\n",
    "    shared_valid_idxs_across_all_criteria = valid_idxs_per_criterion[0]\n",
    "    if len(valid_idxs_per_criterion) > 1:\n",
    "        for i in range(1, len(valid_idxs_per_criterion)):\n",
    "            shared_valid_idxs_across_all_criteria = np.intersect1d(shared_valid_idxs_across_all_criteria, valid_idxs_per_criterion[i])\n",
    "    df_filtered = df.loc[shared_valid_idxs_across_all_criteria, :].copy()\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to specify your filter criteria in a list of tuples, for which each tuple follows this schema:\n",
    "> (column_name, comparison_method, reference_value)\n",
    "\n",
    "So for instance, if you´d like to filter your dataframe by selecting only the data of all freezing bouts, you would add a tuple that specifies that you want all rows from the column \"bout_type\" that have the value \"all_freezing_bouts\": \n",
    "> `('bout_type', 'equal_to', 'all_freezing_bouts')`\n",
    "\n",
    "You can also add more criteria with additional tuples, for instance to filter for specific mouse lines your filter criteria would look like this:\n",
    "> `('line_id', 'is_in_list', ['206', '209'])` \n",
    "\n",
    "Bringing it together, you´d define your filter_criteria in a list of these tuples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "filter_criteria = [('line_id', 'is_in_list', ['206', '209']),\n",
    "                   ('bout_type', 'equal_to', 'all_freezing_bouts')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add as many tuples (= criteria) you´d like. Currently implemented comparison methods include:\n",
    "- \"greater\": selects only rows in which the values of the column are greater than the reference value\n",
    "- \"smaller\": selects only rows in which the values of the column are smaller than the reference value\n",
    "- \"equal_to\": selects only rows in which the values of the column are equal to the reference value\n",
    "- \"is_in_list\": selects only rows in which the values of the column are matching to an element in the reference value (which has to be a list, in this case)\n",
    "- \"is_nan\": selects only rows in which the values of the column are NaN\n",
    "\n",
    "You can then pass the `filter_criteria` along your dataframe to the `filter_dataframe()` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df_filtered = filter_dataframe(df = df, filter_criteria = filter_criteria)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Plotting:\n",
    "\n",
    "Eventually, you can also plot your data, filtering it even more, if you´d like to. When you´re using the `plot()` function, you can specify the following parameters (see function documentation and example usage below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def plot(df: pd.DataFrame, # the DataFrame that contains the data you´d like to plot\n",
    "         x_column: str, # the column name of the data that should be visualized on the x-axis\n",
    "         y_column: str, # the column name of the data that should be visualized on the y-axis\n",
    "         plot_type: str='violinplot', # currently only \"violinplot\" and \"stripplot\" are implemented\n",
    "         hue_column: Optional[str]=None, # if you´d like to use the data of a column to color-code the plotted, you can specify it here (see example below)\n",
    "         hide_legend: bool=True # pass along as `False` if you´d like the legend of the plot to be displayed\n",
    "        ) -> None:\n",
    "    fig = plt.figure(figsize = (8, 5), facecolor = 'white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    if plot_type == 'violinplot':\n",
    "        sns.violinplot(data = df, x = x_column, y = y_column, hue = hue_column)\n",
    "        if df.shape[0] < 10_000:\n",
    "            sns.stripplot(data = df, x = x_column, y = y_column, hue = hue_column, dodge = True, color = 'black', alpha = 0.3)\n",
    "    elif plot_type == 'stripplot':\n",
    "        sns.stripplot(data = df, x = x_column, y = y_column, hue = hue_column, dodge = True)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    if hide_legend:\n",
    "        ax.get_legend().remove()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "plot(df = df_filtered, x_column = 'group_id', y_column = 'total_duration', hue_column = 'week_id')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the following function to create a DataFrame that gives you an overview of your data availability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def check_data_availability(root_dir_path: Path, all_week_ids: List[int], all_paradigm_ids: List[str]) -> pd.DataFrame:\n",
    "    df = pd.DataFrame({}, index=all_week_ids)\n",
    "    root_dir_path = root_dir_path\n",
    "    for week_id in all_week_ids:\n",
    "        for paradigm_id in all_paradigm_ids:\n",
    "            all_matching_filepaths = get_only_matching_xlsx_files(dir_path = root_dir_path, paradigm_id = paradigm_id, week_id = week_id)\n",
    "            for filepath in all_matching_filepaths:\n",
    "                subject_id = get_metadata_from_filename(filepath)['subject_id']\n",
    "                if f'{subject_id}_{paradigm_id}' not in df.columns:\n",
    "                    df[f'{subject_id}_{paradigm_id}'] = False\n",
    "                df.loc[week_id, f'{subject_id}_{paradigm_id}'] = True\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
