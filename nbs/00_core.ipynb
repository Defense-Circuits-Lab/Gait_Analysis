{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Collection of the most central classes of the `gait_analysis` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path, PosixPath\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class EventBout():\n",
    "    \"\"\"\n",
    "    Analysis of the `gait_analysis` package is mainly composed of identifying different\n",
    "    types of events and their combined analysis thereafter. This class represents the\n",
    "    core interface for these analyses, as it bundles all relevant information of such\n",
    "    events.\n",
    "    \n",
    "    Attributes:\n",
    "        self.event_type(str): type of event (e.g. \"freezing\" or \"gait\")\n",
    "        self.event_id(int): index of the occurance of this event type in a recording in chronological order\n",
    "        self.start_idx(int): frame index in which this event bout starts\n",
    "        self.end_idx(int): frame index at which this event bout ends\n",
    "        self.fps(int): frames-per-second of the recording\n",
    "        self.duration(float): duration of the event bout in s\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 event_type: str, # type of event (e.g. \"immobility\" or \"gait\")\n",
    "                 event_id: int, # evnt index (e.g. 0 for the first occurance of this event type in a recording)\n",
    "                 start_idx: int, # index of the frame at which this event bout starts\n",
    "                 end_idx: int, # index of the frame at which this event bout ends\n",
    "                 fps: int # frames-per-second value of the corresponding video recording\n",
    "                ) -> None:\n",
    "        self.event_type = event_type\n",
    "        self.id = event_id\n",
    "        self.start_idx = start_idx\n",
    "        self.end_idx = end_idx\n",
    "        self.fps = fps\n",
    "        self.duration = ((end_idx + 1) - start_idx)/fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\"\"\"\n",
    "ToDo:\n",
    "- reading of metadata and checking whether paradigm_id, week_id, .. are valid needs to be\n",
    "  integrated with the Configs approach, that was already designed for the 3D pipeline\n",
    "  \n",
    "- the abstract methods like \"preprocess\" or \"run_behavrioral_analyses\" should probably also take\n",
    "  some \"configs\" as input to make it consistent for all potential different subclasses? \n",
    "  Yet, at least they are never returning anything, so this would not change.\n",
    "\"\"\"\n",
    "\n",
    "class TrackedRecording(ABC):\n",
    "    \"\"\"\n",
    "    While the analysis depends mainly on the identification of `EventBout`s and their integrated analyses, \n",
    "    `TrackedRecording`s are the data objects on which these events are identified. They are implemented here\n",
    "    as an abstract base class, as they will require slightly different functionalities depending on whether\n",
    "    a 2D or 3D tracking needs to be handled.\n",
    "    \"\"\"\n",
    "    \n",
    "    @property\n",
    "    def valid_paradigms(self) -> List[str]:\n",
    "        return ['OTR', 'OTT', 'OTE']\n",
    "    \n",
    "    @property\n",
    "    def valid_mouse_lines(self) -> List[str]:\n",
    "        return ['194', '195', '196', '206', '209']\n",
    "    \n",
    "    @property\n",
    "    def valid_recording_weeks(self) -> List[int]:\n",
    "        return [1, 4, 8, 12, 14]\n",
    "    \n",
    "    @property\n",
    "    def marker_ids_to_exclude_for_smoothing_and_interpolation(self) -> List[str]:\n",
    "        return ['LED5', 'MazeCornerClosedRight', 'MazeCornerClosedLeft', 'MazeCornerOpenRight', 'MazeCornerOpenLeft']\n",
    "\n",
    "\n",
    "    def __init__(self, \n",
    "                 filepath: Path,  # the filepath to the output of DLC (.h5 or .csv file)\n",
    "                 week_id: int # the experimental week post injection in which the recording was performed. Has to be an element of the self.valid_week_ids list\n",
    "                ) -> None:\n",
    "\n",
    "        assert type(filepath) == PosixPath, '\"filepath\" has to be a pathlib.Path object'\n",
    "        assert week_id in self.valid_recording_weeks, f'\"week_id\" = {week_id} is not listed in \"valid_recording_weeks\": {self.valid_recording_weeks}'\n",
    "        self.df_successfully_loaded, self.loaded_tracking_df = self._attempt_loading_df_from_dlc_output(filepath = filepath)\n",
    "        if self.df_successfully_loaded:\n",
    "            self.filepath = filepath\n",
    "            self.week_id = week_id\n",
    "            self._load_remaining_metadata()\n",
    "            \n",
    "            \n",
    "    @abstractmethod     \n",
    "    def _load_remaining_metadata(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod     \n",
    "    def preprocess(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod     \n",
    "    def run_behavioral_analyses(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod     \n",
    "    def export_results(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod     \n",
    "    def inspect_processing(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _attempt_loading_df_from_dlc_output(self, \n",
    "                                            filepath: Path # filepath to the dlc tracking result (.h5 or .csv file)\n",
    "                                           ) -> Tuple[bool, Optional[pd.DataFrame]]:\n",
    "        assert filepath.name.endswith('.csv') or filepath.name.endswith('.h5'), 'The filepath you specified is not referring to a .csv or a .h5 file!'\n",
    "        try:\n",
    "            if filepath.name.endswith('.csv'):\n",
    "                df = pd.read_csv(filepath, low_memory = False)\n",
    "                df = df.drop('scorer', axis=1)\n",
    "                df.columns = df.iloc[0, :]+ '_' + df.iloc[1, :]\n",
    "                df = df.drop([0, 1], axis=0)\n",
    "                df = df.reset_index()\n",
    "                df = df.drop('index', axis=1)\n",
    "                df = df.astype(float)\n",
    "            else:\n",
    "                df = pd.read_hdf(filepath)\n",
    "                target_column_names = []\n",
    "                for marker_id, data_id in zip(df.columns.get_level_values(1), df.columns.get_level_values(2)):\n",
    "                    target_column_names.append(f'{marker_id}_{data_id}') \n",
    "                df.columns = target_column_names\n",
    "                df = df.astype(float)\n",
    "            successfully_loaded = True\n",
    "        except:\n",
    "            successfully_loaded = False\n",
    "            df = None\n",
    "        return successfully_loaded, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Bodypart:\n",
    "    \"\"\"\n",
    "    Bundles all information and functions needed to process the data of a single bodypart (i.e. marker id).\n",
    "    Requires that the coordinates have already been normalized (translated & rotated) and converted to cm.\n",
    "    \"\"\"\n",
    "    \n",
    "    @property\n",
    "    def exclusion_criteria(self) -> Dict:\n",
    "        return {'likelihood_threshold': 0.5,\n",
    "                              'min_x': -5,\n",
    "                              'max_x': 55,\n",
    "                              'min_y': -3,\n",
    "                              'max_y': 7}\n",
    "\n",
    "    \n",
    "    def __init__(self, \n",
    "                 bodypart_id: str, # The ID of the bodypart, i.e. the marker_id\n",
    "                 df: pd.DataFrame, # The normalized (rotated & translated) & converted (in cm) DataFrame that holds the DLC tracking information about this marker\n",
    "                 fps: int, # frames-per-second at which this recording was acquired\n",
    "                )->None:\n",
    "        self.id = bodypart_id\n",
    "        sliced_df = self._slice_df(df = df)\n",
    "        self.df = self._apply_exclusion_criteria(df = sliced_df, exclusion_criteria = self.exclusion_criteria)\n",
    "        self.fps = fps\n",
    "        self.framerate = 1/fps\n",
    "\n",
    "        \n",
    "    def _slice_df(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_input = df.copy()\n",
    "        data = {'x': df_input.loc[:, f'{self.id}_x'], \n",
    "                'y': df_input.loc[:, f'{self.id}_y'], \n",
    "                'likelihood': df_input.loc[:, f'{self.id}_likelihood']}\n",
    "        return pd.DataFrame(data = data)\n",
    "        \n",
    "        \n",
    "    def _apply_exclusion_criteria(self, df: pd.DataFrame, exclusion_criteria: Dict) -> None:\n",
    "        df.loc[df['likelihood'] < exclusion_criteria['likelihood_threshold'], :] = np.nan\n",
    "        for coord in ['x', 'y']:\n",
    "            df.loc[df[coord] < exclusion_criteria[f'min_{coord}'], :] = np.nan\n",
    "            df.loc[df[coord] > exclusion_criteria[f'max_{coord}'], :] = np.nan\n",
    "        return df\n",
    "\n",
    "        \n",
    "    def calculate_speed_and_identify_immobility(self, sliding_window_size: int, immobility_threshold: float) -> None:\n",
    "        self._add_speed_to_df()\n",
    "        self._add_rolling_speed_to_df(sliding_window_size = sliding_window_size)\n",
    "        self._add_immobility_to_df(immobility_threshold = immobility_threshold)\n",
    "    \n",
    "    \n",
    "    def _add_speed_to_df(self)->None:\n",
    "        self.df.loc[:, 'speed_cm_per_s'] = (self.df.loc[:, 'x'].diff()**2 + self.df.loc[:, 'y'].diff()**2)**0.5 / self.framerate              \n",
    "        \n",
    "    \n",
    "    def _add_rolling_speed_to_df(self, sliding_window_size: int) -> None:\n",
    "        min_periods = int(sliding_window_size * 0.66)\n",
    "        self.df.loc[:, 'rolling_speed_cm_per_s'] = self.df.loc[:, 'speed_cm_per_s'].rolling(sliding_window_size, min_periods = min_periods, center = True).mean()\n",
    "\n",
    "    \n",
    "    def _add_immobility_to_df(self, immobility_threshold: float) -> None:\n",
    "        self.df.loc[:, 'immobility'] = False\n",
    "        self.df.loc[self.df['rolling_speed_cm_per_s'] < immobility_threshold, 'immobility'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
