{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D gait analysis of top cam data\n",
    "\n",
    "> Analysis of DLC tracking data of top camera recordings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp twoD/topcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import Tuple, List, Dict, Optional, Union\n",
    "\n",
    "\n",
    "from gait_analysis.core import TrackedRecording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- rename self.full_df_from_file to self.loaded_tracking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Tracked2DRecording(TrackedRecording):\n",
    "    \"\"\"\n",
    "    Very customized subclass of `TrackedRecording` that was designed to run the gait analysis\n",
    "    on 2D tracking data obtained from a single camera with a top-down-view on the subject.    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def _load_remaining_metadata(self) -> None:\n",
    "        self.fps = self._get_correct_fps()\n",
    "        self.framerate = 1/self.fps\n",
    "        self.metadata = self._retrieve_metadata(filename = filepath.name)\n",
    "        \n",
    "\n",
    "    def _get_correct_fps(self) -> int:\n",
    "        if self.loaded_tracking_df.shape[0] > 25_000:\n",
    "            fps = 80\n",
    "        else:\n",
    "            fps = 30\n",
    "        return fps\n",
    "\n",
    "\n",
    "    def _retrieve_metadata(self, filename: str)->Dict:\n",
    "        \"\"\"\n",
    "        Very much dependent on the following file naming convention:\n",
    "        LLL_FS-SS_YYMMDD_PPP_whatever.h5 (or .csv)\n",
    "        Where:\n",
    "            LLL: three digits mouse line code\n",
    "            FS-SS: the subject ID (must start with capital F) with generation (e.g. F2 for second generation) and the mouse ID as two digits\n",
    "            YYMMDD: the date of the recording as two digits year (YY), month (MM), and day (DD)\n",
    "            PPP: the three letter string of the experimental paradigm\n",
    "        For instance:\n",
    "            196_F7-27_220826_OTT_whatever.h5\n",
    "        \"\"\"\n",
    "        splits = filename.split('_')\n",
    "        line_id, mouse_id, date, paradigm_id, cam_id = splits[0], splits[1], splits[2], splits[3][0:3], 'Top'\n",
    "        self._check_metadata(metadata = (line_id, mouse_id, date, paradigm_id, cam_id))\n",
    "        metadata = {'recording_date': self.recording_date, \n",
    "                    'animal': f'{self.mouse_line}_{self.mouse_id}', \n",
    "                    'paradigm': self.paradigm, \n",
    "                    'cam': self.cam_id}\n",
    "        return metadata\n",
    "    \n",
    "    \n",
    "    # ToDo - replace with something more generalizable that could be put to utils\n",
    "    def _check_metadata(self, \n",
    "                        metadata = Tuple[str]\n",
    "                       ) -> None: \n",
    "        animal_line, animal_id, recording_date, paradigm, cam_id = metadata[0], metadata[1], metadata[2], metadata[3], metadata[4]\n",
    "        self.cam_id = cam_id\n",
    "        if animal_line not in self.valid_mouse_lines:\n",
    "            while True:\n",
    "                entered_input = input(f'Mouse line for {self.filepath}')\n",
    "                if entered_input in self.valid_mouse_lines:\n",
    "                    self.mouse_line = entered_input\n",
    "                    break\n",
    "                else:\n",
    "                    print(f'Entered mouse line does not match any of the defined mouse lines. \\nPlease enter one of the following lines: {self.valid_mouse_lines}')\n",
    "        else:\n",
    "            self.mouse_line = animal_line\n",
    "        if not animal_id.startswith('F'):\n",
    "            while True:\n",
    "                entered_input = input(f'Mouse ID for {self.filepath}')\n",
    "                if entered_input.startswith('F'):\n",
    "                    self.mouse_id = entered_input\n",
    "                    break\n",
    "                else:\n",
    "                    print(f'Animal ID has to start with F. Example: F2-14')\n",
    "        else:\n",
    "            self.mouse_id = animal_id\n",
    "        if paradigm not in self.valid_paradigms:\n",
    "            while True:\n",
    "                entered_input = input(f'Paradigm for {self.filepath}')\n",
    "                if entered_input in self.valid_paradigms:\n",
    "                    self.paradigm = entered_input\n",
    "                    break\n",
    "                else:\n",
    "                    print(f'Entered paradigm does not match any of the defined paradigms. \\nPlease enter one of the following paradigms: {self.valid_paradigms}')\n",
    "        else:\n",
    "            self.paradigm = paradigm\n",
    "        try:\n",
    "            int(recording_date)\n",
    "            self.recording_date = recording_date\n",
    "        except:\n",
    "            while True:\n",
    "                entered_input = input(f'Recording date for {self.filepath}')\n",
    "                try:\n",
    "                    int(recording_date)\n",
    "                    self.recording_date = recording_date\n",
    "                    break\n",
    "                except:\n",
    "                    print(f'Entered recording date has to be an integer in shape YYMMDD. Example: 220812')\n",
    "\n",
    "\n",
    "        def preprocess(self,\n",
    "                   marker_ids_to_compute_coverage: List[str]=['TailBase', 'Snout'], # List of marker_ids on base of which the tracking coverage will be computed\n",
    "                   coverage_threshold: float=0.75, # If coverage of the above defined markers is lower than this threshold, the recording will be excluded from the analysis and therefore not further processed \n",
    "                   max_seconds_to_interpolate: float=0.5, # Maximum time interval in which consecutive nan´s will be interpolated\n",
    "                   likelihood_threshold: float=0.5, # Minimum prediction likelihood of DLC that is required to acceppt predicted marker position as valid \n",
    "                   marker_ids_to_compute_center_of_gravity: List[str]=['TailBase', 'Snout'], # marker ids that will be used to compute the center of gravity\n",
    "                   relative_maze_normalization_error_tolerance: float=0.25 # relative error that is tolerated when estimating the maze position for normalizing it´s position\n",
    "                   ) -> None:\n",
    "        initial_logs_to_add = {'critical_markers_to_compute_coverage': marker_ids_to_compute_coverage,\n",
    "                               'coverage_threshold': coverage_threshold,\n",
    "                               'max_seconds_to_interpolate': max_seconds_to_interpolate,\n",
    "                               'min_likelihood_threshold': likelihood_threshold,\n",
    "                               'center_of_gravity_based_on': marker_ids_to_compute_center_of_gravity, \n",
    "                               'relative_error_tolerance_corner_detection': relative_maze_normalization_error_tolerance}\n",
    "        window_length = self._get_max_odd_n_frames_for_time_interval(fps = self.fps, time_interval = max_seconds_to_interpolate)\n",
    "        marker_ids_to_preprocess = self._get_preprocessing_relevant_marker_ids(df = self.full_df_from_file)\n",
    "        smoothed_df = self._smooth_tracked_coords_and_likelihood(marker_ids = marker_ids_to_preprocess, window_length = window_length, polyorder = 3)\n",
    "        interpolated_df = self._interpolate_low_likelihood_intervals(df = smoothed_df, marker_ids = marker_ids_to_preprocess, max_interval_length = window_length)\n",
    "        interpolated_df_with_cog = self._add_new_marker_derived_from_existing_markers(df = interpolated_df,\n",
    "                                                                                      existing_markers = marker_ids_to_compute_center_of_gravity,\n",
    "                                                                                      new_marker_id = 'CenterOfGravity',\n",
    "                                                                                      likelihood_threshold = likelihood_threshold)\n",
    "        preprocessed_df = self._interpolate_low_likelihood_intervals(df = interpolated_df_with_cog,\n",
    "                                                                     marker_ids = ['CenterOfGravity'],\n",
    "                                                                     max_interval_length = window_length)\n",
    "        coverage_critical_markers = self._compute_coverage(df = preprocessed_df,\n",
    "                                                           critical_marker_ids = marker_ids_to_compute_coverage,\n",
    "                                                           likelihood_threshold = likelihood_threshold)\n",
    "        initial_logs_to_add['coverage_critical_markers'] = coverage_critical_markers\n",
    "        self._add_to_logs(logs_to_add = initial_logs_to_add)\n",
    "        if coverage_critical_markers >= coverage_threshold:\n",
    "            normalization_params = self._get_parameters_to_normalize_maze_coordinates(df = preprocessed_df,\n",
    "                                                                                      relative_error_tolerance = relative_maze_normalization_error_tolerance)\n",
    "            self.normalized_df = self._normalize_df(df = preprocessed_df, normalization_parameters = normalization_params)\n",
    "            self.bodyparts = self._create_bodypart_objects()\n",
    "            normalized_maze_corner_coords = self._get_normalized_maze_corners(normalization_parameters = normalization_params)\n",
    "            coverage_center_of_gravity = self._compute_coverage(df = preprocessed_df,\n",
    "                                                                critical_marker_ids = ['CenterOfGravity'],\n",
    "                                                                likelihood_threshold = likelihood_threshold)\n",
    "            additional_logs_to_add = {'coverage_CenterOfGravity': coverage_center_of_gravity}\n",
    "            for key, value in normalization_params.items():\n",
    "                additional_logs_to_add[key] = value\n",
    "            for key, value in normalized_maze_corner_coords.items():\n",
    "                additional_logs_to_add[f'normalized_{key}_coords'] = value\n",
    "            self._add_to_logs(logs_to_add = additional_logs_to_add)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def test(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
