{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D gait analysis of top cam data\n",
    "\n",
    "> Analysis of DLC tracking data of top camera recordings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp twoD/topcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import Tuple, List, Dict, Optional, Union\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from gait_analysis.core import TrackedRecording\n",
    "from gait_analysis import twoD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Tracked2DRecording(TrackedRecording):\n",
    "    \"\"\"\n",
    "    Very customized subclass of `TrackedRecording` that was designed to run the gait analysis\n",
    "    on 2D tracking data obtained from a single camera with a top-down-view on the subject.    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def _load_remaining_metadata(self) -> None:\n",
    "        self.fps = self._get_correct_fps()\n",
    "        self.framerate = 1/self.fps\n",
    "        self.metadata = self._retrieve_metadata(filename = filepath.name)\n",
    "        \n",
    "\n",
    "    def _get_correct_fps(self) -> int:\n",
    "        if self.loaded_tracking_df.shape[0] > 25_000:\n",
    "            fps = 80\n",
    "        else:\n",
    "            fps = 30\n",
    "        return fps\n",
    "\n",
    "\n",
    "    def _retrieve_metadata(self, filename: str)->Dict:\n",
    "        \"\"\"\n",
    "        Very much dependent on the following file naming convention:\n",
    "        LLL_FS-SS_YYMMDD_PPP_whatever.h5 (or .csv)\n",
    "        Where:\n",
    "            LLL: three digits mouse line code\n",
    "            FS-SS: the subject ID (must start with capital F) with generation (e.g. F2 for second generation) and the mouse ID as two digits\n",
    "            YYMMDD: the date of the recording as two digits year (YY), month (MM), and day (DD)\n",
    "            PPP: the three letter string of the experimental paradigm\n",
    "        For instance:\n",
    "            196_F7-27_220826_OTT_whatever.h5\n",
    "        \"\"\"\n",
    "        splits = filename.split('_')\n",
    "        line_id, mouse_id, date, paradigm_id, cam_id = splits[0], splits[1], splits[2], splits[3][0:3], 'Top'\n",
    "        self._check_metadata(metadata = (line_id, mouse_id, date, paradigm_id, cam_id))\n",
    "        metadata = {'recording_date': self.recording_date, \n",
    "                    'animal': f'{self.mouse_line}_{self.mouse_id}', \n",
    "                    'paradigm': self.paradigm, \n",
    "                    'cam': self.cam_id}\n",
    "        return metadata\n",
    "    \n",
    "    \n",
    "    # ToDo - replace with something more generalizable that could be put to utils\n",
    "    def _check_metadata(self, \n",
    "                        metadata = Tuple[str]\n",
    "                       ) -> None: \n",
    "        animal_line, animal_id, recording_date, paradigm, cam_id = metadata[0], metadata[1], metadata[2], metadata[3], metadata[4]\n",
    "        self.cam_id = cam_id\n",
    "        if animal_line not in self.valid_mouse_lines:\n",
    "            while True:\n",
    "                entered_input = input(f'Mouse line for {self.filepath}')\n",
    "                if entered_input in self.valid_mouse_lines:\n",
    "                    self.mouse_line = entered_input\n",
    "                    break\n",
    "                else:\n",
    "                    print(f'Entered mouse line does not match any of the defined mouse lines. \\nPlease enter one of the following lines: {self.valid_mouse_lines}')\n",
    "        else:\n",
    "            self.mouse_line = animal_line\n",
    "        if not animal_id.startswith('F'):\n",
    "            while True:\n",
    "                entered_input = input(f'Mouse ID for {self.filepath}')\n",
    "                if entered_input.startswith('F'):\n",
    "                    self.mouse_id = entered_input\n",
    "                    break\n",
    "                else:\n",
    "                    print(f'Animal ID has to start with F. Example: F2-14')\n",
    "        else:\n",
    "            self.mouse_id = animal_id\n",
    "        if paradigm not in self.valid_paradigms:\n",
    "            while True:\n",
    "                entered_input = input(f'Paradigm for {self.filepath}')\n",
    "                if entered_input in self.valid_paradigms:\n",
    "                    self.paradigm = entered_input\n",
    "                    break\n",
    "                else:\n",
    "                    print(f'Entered paradigm does not match any of the defined paradigms. \\nPlease enter one of the following paradigms: {self.valid_paradigms}')\n",
    "        else:\n",
    "            self.paradigm = paradigm\n",
    "        try:\n",
    "            int(recording_date)\n",
    "            self.recording_date = recording_date\n",
    "        except:\n",
    "            while True:\n",
    "                entered_input = input(f'Recording date for {self.filepath}')\n",
    "                try:\n",
    "                    int(recording_date)\n",
    "                    self.recording_date = recording_date\n",
    "                    break\n",
    "                except:\n",
    "                    print(f'Entered recording date has to be an integer in shape YYMMDD. Example: 220812')\n",
    "\n",
    "\n",
    "    def preprocess(self,\n",
    "                   marker_ids_to_compute_coverage: List[str]=['TailBase', 'Snout'], # List of marker_ids on base of which the tracking coverage will be computed\n",
    "                   coverage_threshold: float=0.75, # If coverage of the above defined markers is lower than this threshold, the recording will be excluded from the analysis and therefore not further processed \n",
    "                   max_seconds_to_interpolate: float=0.5, # Maximum time interval in which consecutive nan´s will be interpolated\n",
    "                   likelihood_threshold: float=0.5, # Minimum prediction likelihood of DLC that is required to acceppt predicted marker position as valid \n",
    "                   marker_ids_to_compute_center_of_gravity: List[str]=['TailBase', 'Snout'], # marker ids that will be used to compute the center of gravity\n",
    "                   relative_maze_normalization_error_tolerance: float=0.25 # relative error that is tolerated when estimating the maze position for normalizing it´s position\n",
    "                   ) -> None:\n",
    "        \"\"\"\n",
    "        Initiate the preprocessing of the DLC output tracking data. This includes smoothing, \n",
    "        interpolation of small intervals with low likelihoods, and creation of additional markers, \n",
    "        like a center of gravity. Furthermore, the code will identify the most reliable \n",
    "        predicted position of the maze corners and evaluate, whether the reconstructed maze using\n",
    "        these most reliable open corner positions yields a maze that is sufficiently similar to how the\n",
    "        maze should look like (less than a specified relative error threshold). If it meets this\n",
    "        criterion, it will use the open corner from there on to normalize the coordinate system.\n",
    "        If not, it will compute everything just based on the closed corners. Either way, the\n",
    "        coordinate system will be normalized to the closed left corner, applying rotation and\n",
    "        shifting, and eventually convert the coordinates into cm values. As the very last step, \n",
    "        self.bodyparts will be created - a dictionary that contains an `Bodypart` object for each\n",
    "        marker id. Logs will save all parameters specified upon calling the function to make it\n",
    "        easily traceable, which parameter settings were used to obtain a given result (see also\n",
    "        the documentation of the `export_results()` method.\n",
    "        \"\"\"\n",
    "        initial_logs_to_add = {'critical_markers_to_compute_coverage': marker_ids_to_compute_coverage,\n",
    "                               'coverage_threshold': coverage_threshold,\n",
    "                               'max_seconds_to_interpolate': max_seconds_to_interpolate,\n",
    "                               'min_likelihood_threshold': likelihood_threshold,\n",
    "                               'center_of_gravity_based_on': marker_ids_to_compute_center_of_gravity, \n",
    "                               'relative_error_tolerance_corner_detection': relative_maze_normalization_error_tolerance}\n",
    "        window_length = twoD.utils.get_max_odd_n_frames_for_time_interval(fps = self.fps, time_interval = max_seconds_to_interpolate)\n",
    "        marker_ids_to_preprocess = twoD.utils.get_preprocessing_relevant_marker_ids(df = self.loaded_tracking_df,\n",
    "                                                                                    marker_ids_to_exclude = self.marker_ids_to_exclude_for_smoothing_and_interpolation)\n",
    "        smoothed_df = twoD.utils.smooth_tracked_coords_and_likelihood(df = self.loaded_tracking_df, \n",
    "                                                                      window_length = window_length,\n",
    "                                                                      marker_ids = marker_ids_to_preprocess, \n",
    "                                                                      polyorder = 3)\n",
    "        interpolated_df = twoD.utils.interpolate_low_likelihood_intervals(df = smoothed_df, \n",
    "                                                                          marker_ids = marker_ids_to_preprocess, \n",
    "                                                                          max_interval_length = window_length,\n",
    "                                                                          framerate = self.framerate)\n",
    "        interpolated_df_with_cog = twoD.utils.add_new_marker_derived_from_existing_markers(df = interpolated_df,\n",
    "                                                                                      existing_markers = marker_ids_to_compute_center_of_gravity,\n",
    "                                                                                      new_marker_id = 'CenterOfGravity',\n",
    "                                                                                      likelihood_threshold = likelihood_threshold)\n",
    "        preprocessed_df = twoD.utils.interpolate_low_likelihood_intervals(df = interpolated_df_with_cog, \n",
    "                                                                          marker_ids = ['CenterOfGravity'], \n",
    "                                                                          max_interval_length = window_length,\n",
    "                                                                          framerate = self.framerate)        \n",
    "        coverage_critical_markers = twoD.utils.compute_coverage(df = preprocessed_df,\n",
    "                                                                critical_marker_ids = marker_ids_to_compute_coverage,\n",
    "                                                                likelihood_threshold = likelihood_threshold)\n",
    "        initial_logs_to_add['coverage_critical_markers'] = coverage_critical_markers\n",
    "        self._add_to_logs(logs_to_add = initial_logs_to_add)\n",
    "        if coverage_critical_markers >= coverage_threshold:\n",
    "            normalization_params = self._get_parameters_to_normalize_maze_coordinates(df = preprocessed_df,\n",
    "                                                                                      relative_error_tolerance = relative_maze_normalization_error_tolerance)\n",
    "            self.normalized_df = twoD.utils.normalize_df(df = preprocessed_df, normalization_parameters = normalization_params)\n",
    "            self.bodyparts = self._create_bodypart_objects()\n",
    "            normalized_maze_corner_coords = self._get_normalized_maze_corners(normalization_parameters = normalization_params)\n",
    "            coverage_center_of_gravity = twoD.utils.compute_coverage(df = preprocessed_df,\n",
    "                                                                     critical_marker_ids = ['CenterOfGravity'],\n",
    "                                                                     likelihood_threshold = likelihood_threshold)\n",
    "            additional_logs_to_add = {'coverage_CenterOfGravity': coverage_center_of_gravity}\n",
    "            for key, value in normalization_params.items():\n",
    "                additional_logs_to_add[key] = value\n",
    "            for key, value in normalized_maze_corner_coords.items():\n",
    "                additional_logs_to_add[f'normalized_{key}_coords'] = value\n",
    "            self._add_to_logs(logs_to_add = additional_logs_to_add)\n",
    "            \n",
    "    \n",
    "    def _add_to_logs(self,\n",
    "                     logs_to_add: Dict # key-value pairs of things that shall be added to the logs of the recording\n",
    "                    ) -> None:\n",
    "        \"\"\"\n",
    "        At the end of all the processing, the \"logged\" data will be saved in the results .xlsx file.\n",
    "        It thus allows the user to keep track of which settings they used for the respective analysis.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'logs') == False:\n",
    "            self.logs = {}\n",
    "        for key, value in logs_to_add.items():\n",
    "            assert key not in self.logs.keys(), f'{key} is already in self.logs.keys - adding it would result in overwriting the previous entry; please ensure unique naming'\n",
    "            self.logs[key] = value \n",
    "    \n",
    "    \n",
    "    def _get_parameters_to_normalize_maze_coordinates(self, df: pd.DataFrame, relative_error_tolerance: float) -> Dict:\n",
    "        corners = twoD.utils.get_corner_coords_with_likelihoods(df = df)\n",
    "        translation_vector = twoD.utils.get_translation_vector(coords_to_become_origin = corners['MazeCornerClosedLeft']['coords'])\n",
    "        best_result = twoD.utils.evaluate_maze_shape_using_open_corners(corners_and_likelihoods = corners, \n",
    "                                                                        tolerance = relative_error_tolerance)\n",
    "        if best_result['valid']:\n",
    "            side_id = best_result['side_id']\n",
    "            logs_to_add = {'maze_normalization_based_on': f'MazeCornerClosed{side_id}_and_MazeCornerOpen{side_id}'}\n",
    "            conversion_factor = twoD.utils.get_conversion_factor_px_to_cm(coords_point_a = corners[f'MazeCornerClosed{side_id}']['coords'],\n",
    "                                                                         coords_point_b = corners[f'MazeCornerOpen{side_id}']['coords'],\n",
    "                                                                         distance_in_cm = 50)\n",
    "            rotation_angle = twoD.utils.get_rotation_angle_with_open_corner(corners = corners,\n",
    "                                                                           side_id = best_result['side_id'],\n",
    "                                                                           translation_vector = translation_vector,\n",
    "                                                                           conversion_factor = conversion_factor)\n",
    "        else:\n",
    "            logs_to_add = {'maze_normalization_based_on': f'MazeCornerClosedRight_and_MazeCornerClosedLeft'}\n",
    "            conversion_factor = twoD.utils.get_conversion_factor_px_to_cm(coords_point_a = corners['MazeCornerClosedLeft']['coords'],\n",
    "                                                                         coords_point_b = corners['MazeCornerClosedRight']['coords'],\n",
    "                                                                         distance_in_cm = 4)\n",
    "            \n",
    "            \n",
    "            rotation_angle = twoD.utils.get_rotation_angle_with_closed_corners_only(corners = corners,\n",
    "                                                                               translation_vector = translation_vector,\n",
    "                                                                               conversion_factor = conversion_factor)\n",
    "        self._add_to_logs(logs_to_add = logs_to_add)\n",
    "        return {'translation_vector': translation_vector, 'rotation_angle': rotation_angle, 'conversion_factor': conversion_factor}\n",
    "    \n",
    "    \n",
    "    def _create_bodypart_objects(self) -> Dict:\n",
    "        bodyparts = {}\n",
    "        for marker_id in twoD.utils.get_all_unique_marker_ids(df = self.normalized_df):\n",
    "            bodyparts[marker_id] = Bodypart2D(bodypart_id = marker_id, df = self.normalized_df, fps = self.fps)\n",
    "        return bodyparts\n",
    "    \n",
    "    \n",
    "    def _get_normalized_maze_corners(self, normalization_parameters: Dict) -> Dict:\n",
    "        normalized_maze_corner_coordinates = {}\n",
    "        corners = self._get_corner_coords_with_likelihoods(df = self.normalized_df)\n",
    "        for corner_marker_id in corners.keys():\n",
    "            normalized_maze_corner_coordinates[corner_marker_id] = corners[corner_marker_id]['coords']\n",
    "        return normalized_maze_corner_coordinates\n",
    "    \n",
    "    \n",
    "    \n",
    "    def run_event_detection(self,\n",
    "                            bodyparts_critical_for_freezing: List[str]=['Snout', 'CenterOfGravity'],\n",
    "                            bodyparts_for_direction_front_to_back: List[str]=['Snout', 'CenterOfGravity'],\n",
    "                            immobility_max_rolling_speed: float=2.0,\n",
    "                            immobility_min_duration: float=0.1,\n",
    "                            freezing_min_duration: float=0.5,\n",
    "                            gait_min_rolling_speed: float=3.0,\n",
    "                            gait_min_duration: float=0.5,\n",
    "                            gait_disruption_max_time_to_immobility: float=0.15,\n",
    "                            merge_events_max_inbetween_time: float=0.15,\n",
    "                            bodyparts_to_include_in_behavior_df = ['CenterOfGravity', 'Snout', 'TailBase'],\n",
    "                           ) -> None:\n",
    "        # parameter selection for immobility speed threshold and freezing min duration based on:\n",
    "        # https://www.sciencedirect.com/science/article/pii/S0960982216306182\n",
    "        sliding_window_size = int(round(twoD.utils.get_max_odd_n_frames_for_time_interval(fps = self.fps, time_interval = 0.5) / 2, 0))\n",
    "        logs_to_add = {'bodyparts_checked_to_infer_immobility': bodyparts_critical_for_freezing,\n",
    "                       'bodypart_used_to_identify_front': bodyparts_for_direction_front_to_back[0],\n",
    "                       'bodypart_used_to_identify_back': bodyparts_for_direction_front_to_back[1],\n",
    "                       'immobility_max_rolling_speed' : immobility_max_rolling_speed,\n",
    "                       'immobility_min_duration': immobility_min_duration,\n",
    "                       'freezing_min_duration': freezing_min_duration, \n",
    "                       'gait_min_rolling_speed': gait_min_rolling_speed, \n",
    "                       'gait_min_duration': gait_min_duration,\n",
    "                       'gait_disruption_max_time_to_immobility': gait_disruption_max_time_to_immobility,\n",
    "                       #'merge_events_max_inbetween_time': merge_events_max_inbetween_time,\n",
    "                       'sliding_window_size_to_compute_speed': sliding_window_size}\n",
    "        self._add_to_logs(logs_to_add = logs_to_add)\n",
    "        for bodypart in self.bodyparts.values():\n",
    "            bodypart.calculate_speed_and_identify_immobility(sliding_window_size = sliding_window_size, immobility_threshold = immobility_max_rolling_speed)\n",
    "        self.behavior_df = self._create_behavior_df(bodyparts_to_include = bodyparts_to_include_in_behavior_df)\n",
    "        self.behavior_df = twoD.utils.add_orientation_to_behavior_df(behavior_df = self.behavior_df,\n",
    "                                                                     all_bodyparts = self.bodyparts,\n",
    "                                                                     bodyparts_for_direction_front_to_back = bodyparts_for_direction_front_to_back)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self._\n",
    "        self._add_immobility_based_on_several_bodyparts_to_behavior_df(bodyparts_critical_for_freezing = bodyparts_critical_for_freezing)\n",
    "        immobility_events = self._get_immobility_related_events(min_interval_duration = immobility_min_duration, event_type = 'immobility_bout')\n",
    "        self._add_event_bouts_to_behavior_df(event_type = 'immobility_bout', events = immobility_events)\n",
    "        freezing_events = self._get_immobility_related_events(min_interval_duration = freezing_min_duration, event_type = 'freezing_bout')\n",
    "        self._add_event_bouts_to_behavior_df(event_type = 'freezing_bout', events = freezing_events)\n",
    "        gait_events = self._get_gait_events(gait_min_rolling_speed = gait_min_rolling_speed, gait_min_duration = gait_min_duration)\n",
    "        self._add_event_bouts_to_behavior_df(event_type = 'gait_bout', events = gait_events)\n",
    "        gait_disruption_events = self._get_gait_disruption_events(gait_events = gait_events, \n",
    "                                                                  gait_disruption_max_time_to_immobility = gait_disruption_max_time_to_immobility)\n",
    "        self._add_event_bouts_to_behavior_df(event_type = 'gait_disruption_bout', events = gait_disruption_events)  \n",
    "        \n",
    "        \n",
    "    def _create_behavior_df(self, bodyparts_to_include: List[str]) -> pd.DataFrame:\n",
    "        column_names = twoD.utils.get_column_names(df = self.normalized_df, column_identifiers = ['x', 'y', 'likelihood'], marker_ids = bodyparts_to_include)\n",
    "        return self.normalized_df[column_names].copy()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Tracked2DRecording.preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# ToDo: implement properly. Should probably go to core?\n",
    "class Bodypart2D:\n",
    "    \"\"\"\n",
    "    Class that contains information for one single Bodypart.\n",
    "    \n",
    "    Attributes:\n",
    "        self.id(str): Deeplabcut label of the bodypart\n",
    "    \"\"\"\n",
    "    \n",
    "    @property\n",
    "    def exclusion_criteria(self) -> Dict:\n",
    "        return {'likelihood_threshold': 0.5,\n",
    "                              'min_x': -5,\n",
    "                              'max_x': 55,\n",
    "                              'min_y': -3,\n",
    "                              'max_y': 7}\n",
    "\n",
    "    \n",
    "    def __init__(self, bodypart_id: str, df: pd.DataFrame, fps: int)->None:\n",
    "        \"\"\" \n",
    "        Constructor for class Bodypart. \n",
    "        \n",
    "        Since the points in df_raw represent coordinates in the distorted dataframe, we use df_undistort for calculations.\n",
    "        \n",
    "        Parameters:\n",
    "            bodypart_id(str): unique id of marker\n",
    "        \"\"\"\n",
    "        self.id = bodypart_id\n",
    "        sliced_df = self._slice_df(df = df)\n",
    "        self.df = self._apply_exclusion_criteria(df = sliced_df, exclusion_criteria = self.exclusion_criteria)\n",
    "        self.fps = fps\n",
    "        self.framerate = 1/fps\n",
    "\n",
    "        \n",
    "    def _slice_df(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Function, that extracts the coordinates of a single bodypart.\n",
    "        \n",
    "        Parameters:\n",
    "            df(pandas.DataFrame): the full dataframe of the recording with all bodyparts\n",
    "        \"\"\"\n",
    "        df_input = df.copy()\n",
    "        data = {'x': df_input.loc[:, f'{self.id}_x'], \n",
    "                'y': df_input.loc[:, f'{self.id}_y'], \n",
    "                'likelihood': df_input.loc[:, f'{self.id}_likelihood']}\n",
    "        return pd.DataFrame(data = data)\n",
    "        \n",
    "        \n",
    "    def _apply_exclusion_criteria(self, df: pd.DataFrame, exclusion_criteria: Dict) -> None:\n",
    "        df.loc[df['likelihood'] < exclusion_criteria['likelihood_threshold'], :] = np.nan\n",
    "        for coord in ['x', 'y']:\n",
    "            df.loc[df[coord] < exclusion_criteria[f'min_{coord}'], :] = np.nan\n",
    "            df.loc[df[coord] > exclusion_criteria[f'max_{coord}'], :] = np.nan\n",
    "        return df\n",
    "\n",
    "        \n",
    "    def calculate_speed_and_identify_immobility(self, sliding_window_size: int, immobility_threshold: float) -> None:\n",
    "        self._add_speed_to_df()\n",
    "        self._add_rolling_speed_to_df(sliding_window_size = sliding_window_size)\n",
    "        self._add_immobility_to_df(immobility_threshold = immobility_threshold)\n",
    "    \n",
    "    \n",
    "    def _add_speed_to_df(self)->None:\n",
    "        self.df.loc[:, 'speed_cm_per_s'] = (self.df.loc[:, 'x'].diff()**2 + self.df.loc[:, 'y'].diff()**2)**0.5 / self.framerate              \n",
    "        \n",
    "    \n",
    "    def _add_rolling_speed_to_df(self, sliding_window_size: int) -> None:\n",
    "        min_periods = int(sliding_window_size * 0.66)\n",
    "        self.df.loc[:, 'rolling_speed_cm_per_s'] = self.df.loc[:, 'speed_cm_per_s'].rolling(sliding_window_size, min_periods = min_periods, center = True).mean()\n",
    "\n",
    "    \n",
    "    def _add_immobility_to_df(self, immobility_threshold: float) -> None:\n",
    "        self.df.loc[:, 'immobility'] = False\n",
    "        self.df.loc[self.df['rolling_speed_cm_per_s'] < immobility_threshold, 'immobility'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
