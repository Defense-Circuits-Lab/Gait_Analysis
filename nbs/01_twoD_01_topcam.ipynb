{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D gait analysis of top cam data\n",
    "\n",
    "> Analysis of DLC tracking data of top camera recordings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp twoD/topcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import Tuple, List, Dict, Optional, Union\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from gait_analysis.core import TrackedRecording\n",
    "from gait_analysis import twoD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Tracked2DRecording(TrackedRecording):\n",
    "    \"\"\"\n",
    "    Very customized subclass of `TrackedRecording` that was designed to run the gait analysis\n",
    "    on 2D tracking data obtained from a single camera with a top-down-view on the subject.    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def _load_remaining_metadata(self) -> None:\n",
    "        self.fps = self._get_correct_fps()\n",
    "        self.framerate = 1/self.fps\n",
    "        self.metadata = self._retrieve_metadata(filename = filepath.name)\n",
    "        \n",
    "\n",
    "    def _get_correct_fps(self) -> int:\n",
    "        if self.loaded_tracking_df.shape[0] > 25_000:\n",
    "            fps = 80\n",
    "        else:\n",
    "            fps = 30\n",
    "        return fps\n",
    "\n",
    "\n",
    "    def _retrieve_metadata(self, filename: str)->Dict:\n",
    "        \"\"\"\n",
    "        Very much dependent on the following file naming convention:\n",
    "        LLL_FS-SS_YYMMDD_PPP_whatever.h5 (or .csv)\n",
    "        Where:\n",
    "            LLL: three digits mouse line code\n",
    "            FS-SS: the subject ID (must start with capital F) with generation (e.g. F2 for second generation) and the mouse ID as two digits\n",
    "            YYMMDD: the date of the recording as two digits year (YY), month (MM), and day (DD)\n",
    "            PPP: the three letter string of the experimental paradigm\n",
    "        For instance:\n",
    "            196_F7-27_220826_OTT_whatever.h5\n",
    "        \"\"\"\n",
    "        splits = filename.split('_')\n",
    "        line_id, mouse_id, date, paradigm_id, cam_id = splits[0], splits[1], splits[2], splits[3][0:3], 'Top'\n",
    "        self._check_metadata(metadata = (line_id, mouse_id, date, paradigm_id, cam_id))\n",
    "        metadata = {'recording_date': self.recording_date, \n",
    "                    'animal': f'{self.mouse_line}_{self.mouse_id}', \n",
    "                    'paradigm': self.paradigm, \n",
    "                    'cam': self.cam_id}\n",
    "        return metadata\n",
    "    \n",
    "    \n",
    "    # ToDo - replace with something more generalizable that could be put to utils\n",
    "    def _check_metadata(self, \n",
    "                        metadata = Tuple[str]\n",
    "                       ) -> None: \n",
    "        animal_line, animal_id, recording_date, paradigm, cam_id = metadata[0], metadata[1], metadata[2], metadata[3], metadata[4]\n",
    "        self.cam_id = cam_id\n",
    "        if animal_line not in self.valid_mouse_lines:\n",
    "            while True:\n",
    "                entered_input = input(f'Mouse line for {self.filepath}')\n",
    "                if entered_input in self.valid_mouse_lines:\n",
    "                    self.mouse_line = entered_input\n",
    "                    break\n",
    "                else:\n",
    "                    print(f'Entered mouse line does not match any of the defined mouse lines. \\nPlease enter one of the following lines: {self.valid_mouse_lines}')\n",
    "        else:\n",
    "            self.mouse_line = animal_line\n",
    "        if not animal_id.startswith('F'):\n",
    "            while True:\n",
    "                entered_input = input(f'Mouse ID for {self.filepath}')\n",
    "                if entered_input.startswith('F'):\n",
    "                    self.mouse_id = entered_input\n",
    "                    break\n",
    "                else:\n",
    "                    print(f'Animal ID has to start with F. Example: F2-14')\n",
    "        else:\n",
    "            self.mouse_id = animal_id\n",
    "        if paradigm not in self.valid_paradigms:\n",
    "            while True:\n",
    "                entered_input = input(f'Paradigm for {self.filepath}')\n",
    "                if entered_input in self.valid_paradigms:\n",
    "                    self.paradigm = entered_input\n",
    "                    break\n",
    "                else:\n",
    "                    print(f'Entered paradigm does not match any of the defined paradigms. \\nPlease enter one of the following paradigms: {self.valid_paradigms}')\n",
    "        else:\n",
    "            self.paradigm = paradigm\n",
    "        try:\n",
    "            int(recording_date)\n",
    "            self.recording_date = recording_date\n",
    "        except:\n",
    "            while True:\n",
    "                entered_input = input(f'Recording date for {self.filepath}')\n",
    "                try:\n",
    "                    int(recording_date)\n",
    "                    self.recording_date = recording_date\n",
    "                    break\n",
    "                except:\n",
    "                    print(f'Entered recording date has to be an integer in shape YYMMDD. Example: 220812')\n",
    "\n",
    "\n",
    "    def preprocess(self,\n",
    "                   marker_ids_to_compute_coverage: List[str]=['TailBase', 'Snout'], # List of marker_ids on base of which the tracking coverage will be computed\n",
    "                   coverage_threshold: float=0.75, # If coverage of the above defined markers is lower than this threshold, the recording will be excluded from the analysis and therefore not further processed \n",
    "                   max_seconds_to_interpolate: float=0.5, # Maximum time interval in which consecutive nan´s will be interpolated\n",
    "                   likelihood_threshold: float=0.5, # Minimum prediction likelihood of DLC that is required to acceppt predicted marker position as valid \n",
    "                   marker_ids_to_compute_center_of_gravity: List[str]=['TailBase', 'Snout'], # marker ids that will be used to compute the center of gravity\n",
    "                   relative_maze_normalization_error_tolerance: float=0.25 # relative error that is tolerated when estimating the maze position for normalizing it´s position\n",
    "                   ) -> None:\n",
    "        initial_logs_to_add = {'critical_markers_to_compute_coverage': marker_ids_to_compute_coverage,\n",
    "                               'coverage_threshold': coverage_threshold,\n",
    "                               'max_seconds_to_interpolate': max_seconds_to_interpolate,\n",
    "                               'min_likelihood_threshold': likelihood_threshold,\n",
    "                               'center_of_gravity_based_on': marker_ids_to_compute_center_of_gravity, \n",
    "                               'relative_error_tolerance_corner_detection': relative_maze_normalization_error_tolerance}\n",
    "        window_length = twoD.utils.get_max_odd_n_frames_for_time_interval(fps = self.fps, time_interval = max_seconds_to_interpolate)\n",
    "        marker_ids_to_preprocess = twoD.utils.get_preprocessing_relevant_marker_ids(df = self.loaded_tracking_df,\n",
    "                                                                                    marker_ids_to_exclude = self.marker_ids_to_exclude_for_smoothing_and_interpolation)\n",
    "        smoothed_df = twoD.utils.smooth_tracked_coords_and_likelihood(df = self.loaded_tracking_df, \n",
    "                                                                      window_length = window_length,\n",
    "                                                                      marker_ids = marker_ids_to_preprocess, \n",
    "                                                                      polyorder = 3)\n",
    "        interpolated_df = twoD.utils.interpolate_low_likelihood_intervals(df = smoothed_df, \n",
    "                                                                          marker_ids = marker_ids_to_preprocess, \n",
    "                                                                          max_interval_length = window_length,\n",
    "                                                                          framerate = self.framerate)\n",
    "        interpolated_df_with_cog = twoD.utils.add_new_marker_derived_from_existing_markers(df = interpolated_df,\n",
    "                                                                                      existing_markers = marker_ids_to_compute_center_of_gravity,\n",
    "                                                                                      new_marker_id = 'CenterOfGravity',\n",
    "                                                                                      likelihood_threshold = likelihood_threshold)\n",
    "        preprocessed_df = twoD.utils.interpolate_low_likelihood_intervals(df = interpolated_df_with_cog, \n",
    "                                                                          marker_ids = ['CenterOfGravity'], \n",
    "                                                                          max_interval_length = window_length,\n",
    "                                                                          framerate = self.framerate)        \n",
    "        coverage_critical_markers = twoD.utils.compute_coverage(df = preprocessed_df,\n",
    "                                                                critical_marker_ids = marker_ids_to_compute_coverage,\n",
    "                                                                likelihood_threshold = likelihood_threshold)\n",
    "        initial_logs_to_add['coverage_critical_markers'] = coverage_critical_markers\n",
    "        self._add_to_logs(logs_to_add = initial_logs_to_add)\n",
    "        if coverage_critical_markers >= coverage_threshold:\n",
    "            normalization_params = self._get_parameters_to_normalize_maze_coordinates(df = preprocessed_df,\n",
    "                                                                                      relative_error_tolerance = relative_maze_normalization_error_tolerance)\n",
    "            self.normalized_df = twoD.utils.normalize_df(df = preprocessed_df, normalization_parameters = normalization_params)\n",
    "            self.bodyparts = self._create_bodypart_objects()\n",
    "            normalized_maze_corner_coords = self._get_normalized_maze_corners(normalization_parameters = normalization_params)\n",
    "            coverage_center_of_gravity = twoD.utils.compute_coverage(df = preprocessed_df,\n",
    "                                                                     critical_marker_ids = ['CenterOfGravity'],\n",
    "                                                                     likelihood_threshold = likelihood_threshold)\n",
    "            additional_logs_to_add = {'coverage_CenterOfGravity': coverage_center_of_gravity}\n",
    "            for key, value in normalization_params.items():\n",
    "                additional_logs_to_add[key] = value\n",
    "            for key, value in normalized_maze_corner_coords.items():\n",
    "                additional_logs_to_add[f'normalized_{key}_coords'] = value\n",
    "            self._add_to_logs(logs_to_add = additional_logs_to_add)\n",
    "            \n",
    "    \n",
    "    def _add_to_logs(self,\n",
    "                     logs_to_add: Dict # key-value pairs of things that shall be added to the logs of the recording\n",
    "                    ) -> None:\n",
    "        \"\"\"\n",
    "        At the end of all the processing, the \"logged\" data will be saved in the results .xlsx file.\n",
    "        It thus allows the user to keep track of which settings they used for the respective analysis.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'logs') == False:\n",
    "            self.logs = {}\n",
    "        for key, value in logs_to_add.items():\n",
    "            assert key not in self.logs.keys(), f'{key} is already in self.logs.keys - adding it would result in overwriting the previous entry; please ensure unique naming'\n",
    "            self.logs[key] = value \n",
    "    \n",
    "    \n",
    "    def _get_parameters_to_normalize_maze_coordinates(self, df: pd.DataFrame, relative_error_tolerance: float) -> Dict:\n",
    "        corners = twoD.utils.get_corner_coords_with_likelihoods(df = df)\n",
    "        translation_vector = twoD.utils.get_translation_vector(coords_to_become_origin = corners['MazeCornerClosedLeft']['coords'])\n",
    "        best_result = twoD.utils.evaluate_maze_shape_using_open_corners(corners_and_likelihoods = corners, \n",
    "                                                                        tolerance = relative_error_tolerance)\n",
    "        if best_result['valid']:\n",
    "            side_id = best_result['side_id']\n",
    "            logs_to_add = {'maze_normalization_based_on': f'MazeCornerClosed{side_id}_and_MazeCornerOpen{side_id}'}\n",
    "            conversion_factor = twoD.utils.get_conversion_factor_px_to_cm(coords_point_a = corners[f'MazeCornerClosed{side_id}']['coords'],\n",
    "                                                                         coords_point_b = corners[f'MazeCornerOpen{side_id}']['coords'],\n",
    "                                                                         distance_in_cm = 50)\n",
    "            rotation_angle = twoD.utils.get_rotation_angle_with_open_corner(corners = corners,\n",
    "                                                                           side_id = best_result['side_id'],\n",
    "                                                                           translation_vector = translation_vector,\n",
    "                                                                           conversion_factor = conversion_factor)\n",
    "        else:\n",
    "            logs_to_add = {'maze_normalization_based_on': f'MazeCornerClosedRight_and_MazeCornerClosedLeft'}\n",
    "            conversion_factor = twoD.utils.get_conversion_factor_px_to_cm(coords_point_a = corners['MazeCornerClosedLeft']['coords'],\n",
    "                                                                         coords_point_b = corners['MazeCornerClosedRight']['coords'],\n",
    "                                                                         distance_in_cm = 4)\n",
    "            \n",
    "            \n",
    "            rotation_angle = twoD.utils.get_rotation_angle_with_closed_corners_only(corners = corners,\n",
    "                                                                               translation_vector = translation_vector,\n",
    "                                                                               conversion_factor = conversion_factor)\n",
    "        self._add_to_logs(logs_to_add = logs_to_add)\n",
    "        return {'translation_vector': translation_vector, 'rotation_angle': rotation_angle, 'conversion_factor': conversion_factor}\n",
    "    \n",
    "    \n",
    "    def _create_bodypart_objects(self) -> Dict:\n",
    "        bodyparts = {}\n",
    "        for marker_id in twoD.utils.get_all_unique_marker_ids(df = self.normalized_df):\n",
    "            bodyparts[marker_id] = Bodypart2D(bodypart_id = marker_id, df = self.normalized_df, fps = self.fps)\n",
    "        return bodyparts\n",
    "    \n",
    "    \n",
    "    def _get_normalized_maze_corners(self, normalization_parameters: Dict) -> Dict:\n",
    "        normalized_maze_corner_coordinates = {}\n",
    "        corners = self._get_corner_coords_with_likelihoods(df = self.normalized_df)\n",
    "        for corner_marker_id in corners.keys():\n",
    "            normalized_maze_corner_coordinates[corner_marker_id] = corners[corner_marker_id]['coords']\n",
    "        return normalized_maze_corner_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# ToDo: implement properly. Should probably go to core?\n",
    "class Bodypart2D:\n",
    "    \"\"\"\n",
    "    Class that contains information for one single Bodypart.\n",
    "    \n",
    "    Attributes:\n",
    "        self.id(str): Deeplabcut label of the bodypart\n",
    "    \"\"\"\n",
    "    \n",
    "    @property\n",
    "    def exclusion_criteria(self) -> Dict:\n",
    "        return {'likelihood_threshold': 0.5,\n",
    "                              'min_x': -5,\n",
    "                              'max_x': 55,\n",
    "                              'min_y': -3,\n",
    "                              'max_y': 7}\n",
    "\n",
    "    \n",
    "    def __init__(self, bodypart_id: str, df: pd.DataFrame, fps: int)->None:\n",
    "        \"\"\" \n",
    "        Constructor for class Bodypart. \n",
    "        \n",
    "        Since the points in df_raw represent coordinates in the distorted dataframe, we use df_undistort for calculations.\n",
    "        \n",
    "        Parameters:\n",
    "            bodypart_id(str): unique id of marker\n",
    "        \"\"\"\n",
    "        self.id = bodypart_id\n",
    "        sliced_df = self._slice_df(df = df)\n",
    "        self.df = self._apply_exclusion_criteria(df = sliced_df, exclusion_criteria = self.exclusion_criteria)\n",
    "        self.fps = fps\n",
    "        self.framerate = 1/fps\n",
    "\n",
    "        \n",
    "    def _slice_df(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Function, that extracts the coordinates of a single bodypart.\n",
    "        \n",
    "        Parameters:\n",
    "            df(pandas.DataFrame): the full dataframe of the recording with all bodyparts\n",
    "        \"\"\"\n",
    "        df_input = df.copy()\n",
    "        data = {'x': df_input.loc[:, f'{self.id}_x'], \n",
    "                'y': df_input.loc[:, f'{self.id}_y'], \n",
    "                'likelihood': df_input.loc[:, f'{self.id}_likelihood']}\n",
    "        return pd.DataFrame(data = data)\n",
    "        \n",
    "        \n",
    "    def _apply_exclusion_criteria(self, df: pd.DataFrame, exclusion_criteria: Dict) -> None:\n",
    "        df.loc[df['likelihood'] < exclusion_criteria['likelihood_threshold'], :] = np.nan\n",
    "        for coord in ['x', 'y']:\n",
    "            df.loc[df[coord] < exclusion_criteria[f'min_{coord}'], :] = np.nan\n",
    "            df.loc[df[coord] > exclusion_criteria[f'max_{coord}'], :] = np.nan\n",
    "        return df\n",
    "\n",
    "        \n",
    "    def calculate_speed_and_identify_immobility(self, sliding_window_size: int, immobility_threshold: float) -> None:\n",
    "        self._add_speed_to_df()\n",
    "        self._add_rolling_speed_to_df(sliding_window_size = sliding_window_size)\n",
    "        self._add_immobility_to_df(immobility_threshold = immobility_threshold)\n",
    "    \n",
    "    \n",
    "    def _add_speed_to_df(self)->None:\n",
    "        self.df.loc[:, 'speed_cm_per_s'] = (self.df.loc[:, 'x'].diff()**2 + self.df.loc[:, 'y'].diff()**2)**0.5 / self.framerate              \n",
    "        \n",
    "    \n",
    "    def _add_rolling_speed_to_df(self, sliding_window_size: int) -> None:\n",
    "        min_periods = int(sliding_window_size * 0.66)\n",
    "        self.df.loc[:, 'rolling_speed_cm_per_s'] = self.df.loc[:, 'speed_cm_per_s'].rolling(sliding_window_size, min_periods = min_periods, center = True).mean()\n",
    "\n",
    "    \n",
    "    def _add_immobility_to_df(self, immobility_threshold: float) -> None:\n",
    "        self.df.loc[:, 'immobility'] = False\n",
    "        self.df.loc[self.df['rolling_speed_cm_per_s'] < immobility_threshold, 'immobility'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
