{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# twoD/utils\n",
    "\n",
    "> Basic functions used throughout the 2D module and/or that foster the use of this module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp twoD/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from gait_analysis.core import EventBout\n",
    "from gait_analysis.twoD import cam_interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# ToDo: If additional camera interfaces will be implemented, they should be specifieable here\n",
    "\n",
    "def process_all_dlc_tracking_h5s_with_default_settings(in_dir_path: Path, # path to the input directory which contains all DLC tracking data results\n",
    "                                                       out_dir_path: Path, # path to the output directory where all processed results will be saved\n",
    "                                                       week_id: int, # number of weeks post injection\n",
    "                                                      ) -> None:\n",
    "    filepaths_dlc_trackings = []\n",
    "    for filepath in in_dir_path.iterdir():\n",
    "        if filepath.name.endswith('.h5'):\n",
    "            if 'filtered' not in filepath.name:\n",
    "                filepaths_dlc_trackings.append(filepath)\n",
    "    for filepath in tqdm(filepaths_dlc_trackings):\n",
    "        recording = cam_interfaces.TopTracked2DRecording(filepath = filepath, week_id = week_id)\n",
    "        if recording.df_successfully_loaded:\n",
    "            recording.preprocess_tracking()\n",
    "            if recording.logs['coverage_critical_markers'] >= recording.logs['coverage_threshold']: \n",
    "                recording.run_event_detection()\n",
    "                recording.export_results(out_dir_path = out_dir_path)\n",
    "                recording.inspect_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_max_odd_n_frames_for_time_interval(fps: int, # frames per second of the recording\n",
    "                                           time_interval: 0.5 # desired maximal time interval in seconds; default = 0.5 s\n",
    "                                          ) -> int:\n",
    "    \"\"\"\n",
    "    For the savgol_filter function of scipy - which will be used during preprocessing to smooth the data -\n",
    "    you need an odd integer as the window_length parameter. This function helps to find the maximum odd number\n",
    "    of frames that still fit within a specified time interval at a given fps.\n",
    "    \"\"\"\n",
    "    assert type(fps) == int, '\"fps\" has to be an integer!'\n",
    "    frames_per_time_interval = fps * time_interval\n",
    "    if frames_per_time_interval % 2 == 0:\n",
    "        max_odd_frame_count = frames_per_time_interval - 1\n",
    "    elif frames_per_time_interval == int(frames_per_time_interval):\n",
    "        max_odd_frame_count = frames_per_time_interval\n",
    "    else:\n",
    "        frames_per_time_interval = int(frames_per_time_interval)\n",
    "        if frames_per_time_interval % 2 == 0:\n",
    "            max_odd_frame_count = frames_per_time_interval - 1\n",
    "        else:\n",
    "            max_odd_frame_count = frames_per_time_interval\n",
    "    assert max_odd_frame_count > 0, f'The specified time interval is too short to fit an odd number of frames'\n",
    "    return int(max_odd_frame_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_column_names(df: pd.DataFrame, \n",
    "                     column_identifiers: List[str], \n",
    "                     marker_ids: List[str]=['all'],\n",
    "                    ) -> List[str]:\n",
    "    matching_column_names = []\n",
    "    for column_name in df.columns:\n",
    "        marker_id, column_identifier = column_name.split('_')\n",
    "        if marker_ids == ['all']:\n",
    "            if column_identifier in column_identifiers:\n",
    "                matching_column_names.append(column_name)\n",
    "        else:\n",
    "            if (marker_id in marker_ids) and (column_identifier in column_identifiers):\n",
    "                matching_column_names.append(column_name)\n",
    "    return matching_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_interval_border_idxs(all_matching_idxs: np.ndarray,\n",
    "                              framerate: float,\n",
    "                              min_interval_duration: Optional[float]=None, \n",
    "                              max_interval_duration: Optional[float]=None,\n",
    "                             ) -> List[Tuple[int, int]]:\n",
    "    interval_border_idxs = []\n",
    "    if all_matching_idxs.shape[0] >= 1:\n",
    "        step_idxs = np.where(np.diff(all_matching_idxs) > 1)[0]\n",
    "        step_end_idxs = np.concatenate([step_idxs, np.array([all_matching_idxs.shape[0] - 1])])\n",
    "        step_start_idxs = np.concatenate([np.array([0]), step_idxs + 1])\n",
    "        interval_start_idxs = all_matching_idxs[step_start_idxs]\n",
    "        interval_end_idxs = all_matching_idxs[step_end_idxs]\n",
    "        for start_idx, end_idx in zip(interval_start_idxs, interval_end_idxs):\n",
    "            interval_frame_count = (end_idx+1) - start_idx\n",
    "            interval_duration = interval_frame_count * framerate          \n",
    "            if (min_interval_duration != None) and (max_interval_duration != None):\n",
    "                append_interval = min_interval_duration <= interval_duration <= max_interval_duration \n",
    "            elif min_interval_duration != None:\n",
    "                append_interval = min_interval_duration <= interval_duration\n",
    "            elif max_interval_duration != None:\n",
    "                append_interval = interval_duration <= max_interval_duration\n",
    "            else:\n",
    "                append_interval = True\n",
    "            if append_interval:\n",
    "                interval_border_idxs.append((start_idx, end_idx))\n",
    "    return interval_border_idxs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def compute_coverage(df: pd.DataFrame,\n",
    "                     critical_marker_ids: List[str],\n",
    "                     likelihood_threshold: float=0.5\n",
    "                    ) -> float:\n",
    "    idxs_where_all_markers_exceed_likelihood_threshold = get_idxs_where_all_markers_exceed_likelihood(df = df, \n",
    "                                                                                                    marker_ids = critical_marker_ids,\n",
    "                                                                                                    likelihood_threshold = likelihood_threshold)\n",
    "    return idxs_where_all_markers_exceed_likelihood_threshold.shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_idxs_where_all_markers_exceed_likelihood(df: pd.DataFrame,\n",
    "                                                  marker_ids: List[str],\n",
    "                                                  likelihood_threshold: float=0.5\n",
    "                                                 ) -> np.ndarray:\n",
    "    valid_idxs_per_marker_id = []\n",
    "    for marker_id in marker_ids:\n",
    "        valid_idxs_per_marker_id.append(df.loc[df[f'{marker_id}_likelihood'] >= likelihood_threshold].index.values)\n",
    "    shared_valid_idxs_for_all_markers = valid_idxs_per_marker_id[0]\n",
    "    if len(valid_idxs_per_marker_id) > 1:\n",
    "        for i in range(1, len(valid_idxs_per_marker_id)):\n",
    "            shared_valid_idxs_for_all_markers = np.intersect1d(shared_valid_idxs_for_all_markers, valid_idxs_per_marker_id[i])\n",
    "    return shared_valid_idxs_for_all_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
