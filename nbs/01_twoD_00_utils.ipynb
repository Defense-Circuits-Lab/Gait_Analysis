{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utility functions for two dimensional gait analyses\n",
    "\n",
    "> Basic functions used throughout the 2D module and/or that foster the use of this module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp twoD/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "#from gait_analysis.twoD.topcam import Tracked2DRecording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def process_all_dlc_tracking_h5s_with_default_settings(in_dir_path: Path, # path to the input directory which contains all DLC tracking data results\n",
    "                                                       week_id: int, # number of weeks post injection\n",
    "                                                       out_dir_path: Path # path to the output directory where all processed results will be saved\n",
    "                                                      ) -> None:\n",
    "    filepaths_dlc_trackings = []\n",
    "    for filepath in in_dir_path.iterdir():\n",
    "        if filepath.name.endswith('.h5'):\n",
    "            if 'filtered' not in filepath.name:\n",
    "                filepaths_dlc_trackings.append(filepath)\n",
    "    for filepath in tqdm(filepaths_dlc_trackings):\n",
    "        recording = Tracked2DRecording(filepath = filepath, week_id = week_id)\n",
    "        if recording.df_successfully_loaded:\n",
    "            recording.preprocess()\n",
    "            if recording.logs['coverage_critical_markers'] >= recording.logs['coverage_threshold']: \n",
    "                recording.run_behavioral_analyses()\n",
    "                recording.export_results(out_dir_path = out_dir_path)\n",
    "                recording.inspect_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions related to preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_max_odd_n_frames_for_time_interval(fps: int, # frames per second of the recording\n",
    "                                           time_interval: 0.5 # desired maximal time interval in seconds; default = 0.5 s\n",
    "                                          ) -> int:\n",
    "    \"\"\"\n",
    "    For the savgol_filter function of scipy - which will be used during preprocessing to smooth the data -\n",
    "    you need an odd integer as the window_length parameter. This function helps to find the maximum odd number\n",
    "    of frames that still fit within a specified time interval at a given fps.\n",
    "    \"\"\"\n",
    "    assert type(fps) == int, '\"fps\" has to be an integer!'\n",
    "    frames_per_time_interval = fps * time_interval\n",
    "    if frames_per_time_interval % 2 == 0:\n",
    "        max_odd_frame_count = frames_per_time_interval - 1\n",
    "    elif frames_per_time_interval == int(frames_per_time_interval):\n",
    "        max_odd_frame_count = frames_per_time_interval\n",
    "    else:\n",
    "        frames_per_time_interval = int(frames_per_time_interval)\n",
    "        if frames_per_time_interval % 2 == 0:\n",
    "            max_odd_frame_count = frames_per_time_interval - 1\n",
    "        else:\n",
    "            max_odd_frame_count = frames_per_time_interval\n",
    "    assert max_odd_frame_count > 0, f'The specified time interval is too short to fit an odd number of frames'\n",
    "    return int(max_odd_frame_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_preprocessing_relevant_marker_ids(df: pd.DataFrame, # DataFrame with x, y, and likelihood for tracked marker_ids\n",
    "                                          marker_ids_to_exclude: Optional[List[str]]=None # list of marker_ids to exclude; optional default None\n",
    "                                         ) -> List[str]:\n",
    "    all_marker_ids = get_all_unique_marker_ids(df = df)\n",
    "    relevant_marker_ids = all_marker_ids\n",
    "    if marker_ids_to_exclude != None:\n",
    "        for marker_id_to_exclude in marker_ids_to_exclude:\n",
    "            if marker_id_to_exclude in relevant_marker_ids:\n",
    "                relevant_marker_ids.remove(marker_id_to_exclude)\n",
    "    return relevant_marker_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_all_unique_marker_ids(df: pd.DataFrame) -> List[str]:\n",
    "    unique_marker_ids = []\n",
    "    for column_name in df.columns:\n",
    "        marker_id, _ = column_name.split('_')\n",
    "        if marker_id not in unique_marker_ids:\n",
    "            unique_marker_ids.append(marker_id)\n",
    "    return unique_marker_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def smooth_tracked_coords_and_likelihood(df: pd.DataFrame, # DataFrame to smooth\n",
    "                                         window_length: int, # Odd integer (!) of sliding window size in frames to consider for smoothing\n",
    "                                         marker_ids: List[str]=['all'], # List of markers that will be smoothed; optional default ['all'] to smooth all marker_ids\n",
    "                                         polyorder: int=3 # Order of the polynom used for the savgol filter\n",
    "                                        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Smoothes the DataFrame basically using the implementation from DLC2kinematics:\n",
    "    https://github.com/AdaptiveMotorControlLab/DLC2Kinematics/blob/82e7e60e00e0efb3c51e024c05a5640c91032026/src/dlc2kinematics/preprocess.py#L64\n",
    "    However, with one key change: likelihoods will also be smoothed.\n",
    "    In addition, we will not smooth the columns for the tracked LEDs and the MazeCorners.\n",
    "\n",
    "    Note: window_length has to be an odd integer!\n",
    "    \"\"\"\n",
    "    smoothed_df = df.copy()\n",
    "    column_names = get_column_names(df = smoothed_df,\n",
    "                                    column_identifiers = ['x', 'y', 'likelihood'],\n",
    "                                    marker_ids = marker_ids)\n",
    "    column_idxs_to_smooth = smoothed_df.columns.get_indexer(column_names)\n",
    "    smoothed_df.iloc[:, column_idxs_to_smooth] = savgol_filter(x = smoothed_df.iloc[:, column_idxs_to_smooth],\n",
    "                                                               window_length = window_length,\n",
    "                                                               polyorder = polyorder,\n",
    "                                                               axis = 0)\n",
    "    return smoothed_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_column_names(df: pd.DataFrame, \n",
    "                     column_identifiers: List[str], \n",
    "                     marker_ids: List[str]=['all'],\n",
    "                    ) -> List[str]:\n",
    "    matching_column_names = []\n",
    "    for column_name in df.columns:\n",
    "        marker_id, column_identifier = column_name.split('_')\n",
    "        if marker_ids == ['all']:\n",
    "            if column_identifier in column_identifiers:\n",
    "                matching_column_names.append(column_name)\n",
    "        else:\n",
    "            if (marker_id in marker_ids) and (column_identifier in column_identifiers):\n",
    "                matching_column_names.append(column_name)\n",
    "    return matching_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def interpolate_low_likelihood_intervals(df: pd.DataFrame, \n",
    "                                         marker_ids: List[str], \n",
    "                                         max_interval_length: int,\n",
    "                                         framerate: float,\n",
    "                                        ) -> pd.DataFrame:\n",
    "    interpolated_df = df.copy()\n",
    "    for marker_id in marker_ids:\n",
    "        low_likelihood_interval_border_idxs = get_low_likelihood_interval_border_idxs(likelihood_series = interpolated_df[f'{marker_id}_likelihood'], \n",
    "                                                                                      max_interval_length = max_interval_length, \n",
    "                                                                                      framerate = framerate)\n",
    "        for start_idx, end_idx in low_likelihood_interval_border_idxs:\n",
    "            if (start_idx - 1 >= 0) and (end_idx + 2 < interpolated_df.shape[0]):\n",
    "                interpolated_df[f'{marker_id}_x'][start_idx - 1 : end_idx + 2] = interpolated_df[f'{marker_id}_x'][start_idx - 1 : end_idx + 2].interpolate()\n",
    "                interpolated_df[f'{marker_id}_y'][start_idx - 1 : end_idx + 2] = interpolated_df[f'{marker_id}_y'][start_idx - 1 : end_idx + 2].interpolate()\n",
    "                interpolated_df[f'{marker_id}_likelihood'][start_idx : end_idx + 1] = 0.5\n",
    "    return interpolated_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_low_likelihood_interval_border_idxs(likelihood_series: pd.Series,\n",
    "                                            framerate: float,\n",
    "                                            max_interval_length: int,\n",
    "                                            min_likelihood_threshold: float=0.5\n",
    "                                           ) -> List[Tuple[int, int]]:\n",
    "    all_low_likelihood_idxs = np.where(likelihood_series.values < min_likelihood_threshold)[0]\n",
    "    short_low_likelihood_interval_border_idxs = get_interval_border_idxs(all_matching_idxs = all_low_likelihood_idxs,\n",
    "                                                                               framerate = framerate,\n",
    "                                                                               max_interval_duration = max_interval_length*framerate)\n",
    "    return short_low_likelihood_interval_border_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_interval_border_idxs(all_matching_idxs: np.ndarray,\n",
    "                              framerate: float,\n",
    "                              min_interval_duration: Optional[float]=None, \n",
    "                              max_interval_duration: Optional[float]=None,\n",
    "                             ) -> List[Tuple[int, int]]:\n",
    "    interval_border_idxs = []\n",
    "    if all_matching_idxs.shape[0] >= 1:\n",
    "        step_idxs = np.where(np.diff(all_matching_idxs) > 1)[0]\n",
    "        step_end_idxs = np.concatenate([step_idxs, np.array([all_matching_idxs.shape[0] - 1])])\n",
    "        step_start_idxs = np.concatenate([np.array([0]), step_idxs + 1])\n",
    "        interval_start_idxs = all_matching_idxs[step_start_idxs]\n",
    "        interval_end_idxs = all_matching_idxs[step_end_idxs]\n",
    "        for start_idx, end_idx in zip(interval_start_idxs, interval_end_idxs):\n",
    "            interval_frame_count = (end_idx+1) - start_idx\n",
    "            interval_duration = interval_frame_count * framerate          \n",
    "            if (min_interval_duration != None) and (max_interval_duration != None):\n",
    "                append_interval = min_interval_duration <= interval_duration <= max_interval_duration \n",
    "            elif min_interval_duration != None:\n",
    "                append_interval = min_interval_duration <= interval_duration\n",
    "            elif max_interval_duration != None:\n",
    "                append_interval = interval_duration <= max_interval_duration\n",
    "            else:\n",
    "                append_interval = True\n",
    "            if append_interval:\n",
    "                interval_border_idxs.append((start_idx, end_idx))\n",
    "    return interval_border_idxs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def add_new_marker_derived_from_existing_markers(df: pd.DataFrame,\n",
    "                                                 existing_markers: List[str],\n",
    "                                                 new_marker_id: str,\n",
    "                                                 likelihood_threshold: float = 0.5\n",
    "                                                )->None:\n",
    "    df_with_new_marker = df.copy()\n",
    "    for coordinate in ['x', 'y']:\n",
    "        df_with_new_marker[f'{new_marker_id}_{coordinate}'] = (sum([df_with_new_marker[f'{marker_id}_{coordinate}'] for marker_id in existing_markers]))/len(existing_markers)\n",
    "    df_with_new_marker[f'{new_marker_id}_likelihood'] = 0\n",
    "    row_idxs_where_all_likelihoods_exceeded_threshold = get_idxs_where_all_markers_exceed_likelihood(df = df_with_new_marker, \n",
    "                                                                                                       marker_ids = existing_markers, \n",
    "                                                                                                       likelihood_threshold = 0.5)\n",
    "    df_with_new_marker.iloc[row_idxs_where_all_likelihoods_exceeded_threshold, -1] = 1\n",
    "    return df_with_new_marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_idxs_where_all_markers_exceed_likelihood(df: pd.DataFrame,\n",
    "                                                  marker_ids: List[str],\n",
    "                                                  likelihood_threshold: float=0.5\n",
    "                                                 ) -> np.ndarray:\n",
    "    valid_idxs_per_marker_id = []\n",
    "    for marker_id in marker_ids:\n",
    "        valid_idxs_per_marker_id.append(df.loc[df[f'{marker_id}_likelihood'] >= likelihood_threshold].index.values)\n",
    "    shared_valid_idxs_for_all_markers = valid_idxs_per_marker_id[0]\n",
    "    if len(valid_idxs_per_marker_id) > 1:\n",
    "        for i in range(1, len(valid_idxs_per_marker_id)):\n",
    "            shared_valid_idxs_for_all_markers = np.intersect1d(shared_valid_idxs_for_all_markers, valid_idxs_per_marker_id[i])\n",
    "    return shared_valid_idxs_for_all_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def compute_coverage(df: pd.DataFrame,\n",
    "                     critical_marker_ids: List[str],\n",
    "                     likelihood_threshold: float=0.5\n",
    "                    ) -> float:\n",
    "    idxs_where_all_markers_exceed_likelihood_threshold = get_idxs_where_all_markers_exceed_likelihood(df = df, \n",
    "                                                                                                    marker_ids = critical_marker_ids,\n",
    "                                                                                                    likelihood_threshold = likelihood_threshold)\n",
    "    return idxs_where_all_markers_exceed_likelihood_threshold.shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_corner_coords_with_likelihoods(df: pd.DataFrame) -> Dict:\n",
    "    corner_coords_with_likelihood = {}\n",
    "    for corner_marker_id in ['MazeCornerClosedRight', 'MazeCornerClosedLeft', 'MazeCornerOpenRight', 'MazeCornerOpenLeft']:\n",
    "        xy_coords, min_likelihood = get_most_reliable_marker_position_with_likelihood(df = df, marker_id = corner_marker_id)\n",
    "        corner_coords_with_likelihood[corner_marker_id] = {'coords': xy_coords, 'min_likelihood': min_likelihood}\n",
    "    return corner_coords_with_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_most_reliable_marker_position_with_likelihood(df: pd.DataFrame,\n",
    "                                                      marker_id: str,\n",
    "                                                      percentile: float=99.95\n",
    "                                                     ) -> Tuple[np.array, float]:\n",
    "    likelihood_threshold = np.nanpercentile(df[f'{marker_id}_likelihood'].values, percentile)\n",
    "    df_most_reliable_frames = df.loc[df[f'{marker_id}_likelihood'] >= likelihood_threshold].copy()\n",
    "    most_reliable_x, most_reliable_y = df_most_reliable_frames[f'{marker_id}_x'].median(), df_most_reliable_frames[f'{marker_id}_y'].median()\n",
    "    return np.array([most_reliable_x, most_reliable_y]), likelihood_threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_translation_vector(coords_to_become_origin: np.ndarray) -> np.ndarray:\n",
    "    return -coords_to_become_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def evaluate_maze_shape_using_open_corners(corners_and_likelihoods: Dict, tolerance: float) -> Dict:\n",
    "    best_result = {'valid': False, 'mean_error': tolerance + 1, 'open_corner_id': None, 'side_id': None}\n",
    "    all_open_corner_marker_ids = [corner_marker_id for corner_marker_id in corners_and_likelihoods.keys() if 'Open' in corner_marker_id]\n",
    "    for open_corner_marker_id in all_open_corner_marker_ids:\n",
    "        valid_positions = False\n",
    "        side_id = open_corner_marker_id[open_corner_marker_id.find('Open') + 4:]\n",
    "        if side_id == 'Left': opposite_side_id = 'Right'\n",
    "        else: opposite_side_id = 'Left'\n",
    "        closed_corner_opposite_side = f'MazeCornerClosed{opposite_side_id}'\n",
    "        angle_error = compute_angle_error(a = corners_and_likelihoods[f'MazeCornerClosed{opposite_side_id}']['coords'],\n",
    "                                          b = corners_and_likelihoods[f'MazeCornerClosed{side_id}']['coords'],\n",
    "                                          c = corners_and_likelihoods[open_corner_marker_id]['coords'])\n",
    "        distance_ratio_error = compute_distance_ratio_error(corners_and_likelihoods = corners_and_likelihoods,\n",
    "                                                            open_corner_marker_id = open_corner_marker_id,\n",
    "                                                            side_id = side_id)\n",
    "        if (angle_error <= tolerance) & (distance_ratio_error <= tolerance):\n",
    "            valid_positions = True\n",
    "        mean_error = (angle_error + distance_ratio_error) / 2\n",
    "        if mean_error < best_result['mean_error']:\n",
    "            best_result['valid'] = valid_positions\n",
    "            best_result['mean_error'] = mean_error\n",
    "            best_result['open_corner_id'] = open_corner_marker_id\n",
    "            best_result['side_id'] = side_id\n",
    "    return best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def compute_error_proportion(query_value: float, target_value: float) -> float:\n",
    "    return abs(query_value - target_value) / target_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def compute_angle_error(a: np.ndarray, b: np.ndarray, c: np.ndarray) -> float:\n",
    "    # b is point at the joint that connects the other two\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.degrees(np.arccos(cosine_angle))\n",
    "    return compute_error_proportion(query_value = angle, target_value = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def compute_distance_ratio_error(corners_and_likelihoods: Dict, open_corner_marker_id: str, side_id: str) -> float:\n",
    "    maze_width = et_distance_between_two_points(corners_and_likelihoods['MazeCornerClosedLeft']['coords'],\n",
    "                                                       corners_and_likelihoods['MazeCornerClosedRight']['coords'])\n",
    "    maze_length = get_distance_between_two_points(corners_and_likelihoods[f'MazeCornerClosed{side_id}']['coords'],\n",
    "                                                       corners_and_likelihoods[open_corner_marker_id]['coords'])\n",
    "    distance_ratio = maze_length/maze_width\n",
    "    return compute_error_proportion(query_value = distance_ratio, target_value = 50/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_distance_between_two_points(coords_point_a: np.ndarray, coords_point_b: np.ndarray) -> float:\n",
    "    return ((coords_point_a[0] - coords_point_b[0])**2 + (coords_point_a[1] - coords_point_b[1])**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_conversion_factor_px_to_cm(coords_point_a: np.ndarray, coords_point_b: np.ndarray, distance_in_cm: float) -> float:\n",
    "    distance = get_distance_between_two_points(coords_point_a, coords_point_b)\n",
    "    return distance_in_cm / distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_rotation_angle_with_open_corner(corners: Dict, side_id: str, translation_vector: np.ndarray, conversion_factor: float) -> float:\n",
    "    \"\"\"\n",
    "    Function, that calculates the rotation angle of the maze considering the best matching open corner\n",
    "    and the corresponding closed corner on the same side.\n",
    "\n",
    "    Returns:\n",
    "        float: angle in radians\n",
    "    \"\"\"\n",
    "    if side_id == 'Left':\n",
    "        side_specific_y = 0\n",
    "    else:\n",
    "        side_specific_y = 4\n",
    "    translated_closed_corner = corners[f'MazeCornerClosed{side_id}']['coords'] + translation_vector\n",
    "    translated_open_corner = corners[f'MazeCornerOpen{side_id}']['coords'] + translation_vector\n",
    "    target_rotated_open_corner = np.asarray([50 / conversion_factor, side_specific_y / conversion_factor])\n",
    "    length_a = get_distance_between_two_points(translated_open_corner, target_rotated_open_corner) * conversion_factor\n",
    "    length_b = get_distance_between_two_points(translated_open_corner, translated_closed_corner) * conversion_factor\n",
    "    length_c = 50\n",
    "    angle = math.acos((length_b**2 + length_c**2 - length_a**2) / (2 * length_b * length_c))\n",
    "    return angle  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_rotation_angle_with_closed_corners_only(corners: Dict, translation_vector: np.ndarray, conversion_factor: float) -> float:\n",
    "    translated_closed_left = corners['MazeCornerClosedLeft']['coords'] + translation_vector\n",
    "    translated_closed_right = corners['MazeCornerClosedRight']['coords'] + translation_vector\n",
    "    target_rotated_closed_right = np.asarray([0, 4 / conversion_factor])\n",
    "\n",
    "    length_a = get_distance_between_two_points(translated_closed_right, target_rotated_closed_right) * conversion_factor\n",
    "    length_b = get_distance_between_two_points(translated_closed_left, translated_closed_right) * conversion_factor\n",
    "    length_c = 4\n",
    "    angle = math.acos((length_b**2 + length_c**2 - length_a**2) / (2 * length_b * length_c))\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def normalize_df(df: pd.DataFrame, normalization_parameters)->None:\n",
    "    unadjusted_df = df.copy()\n",
    "    translated_df = translate_df(df = unadjusted_df, translation_vector = normalization_parameters['translation_vector'])\n",
    "    rotated_and_translated_df = self._rotate_df(df = translated_df, rotation_angle = normalization_parameters['rotation_angle'])\n",
    "    final_df = self._convert_df_to_cm(df = rotated_and_translated_df, conversion_factor = normalization_parameters['conversion_factor'])\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def translate_df(df: pd.DataFrame, translation_vector: np.array) -> pd.DataFrame:\n",
    "    for marker_id in twoD.utils.get_all_unique_marker_ids(df = df):\n",
    "        df.loc[:, [f'{marker_id}_x', f'{marker_id}_y']] += translation_vector\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def rotate_df(df: pd.DataFrame, # DataFrame with 2D coordinates to be rotated\n",
    "              rotation_angle: float # rotation angle in radians\n",
    "             ) -> pd.DataFrame:\n",
    "    df_rotated = df.copy()\n",
    "    cos_theta, sin_theta = math.cos(rotation_angle), math.sin(rotation_angle)\n",
    "    for marker_id in twoD.utils.get_all_unique_marker_ids(df = df):\n",
    "        df_rotated[f'{marker_id}_x'] = df[f'{marker_id}_x'] * cos_theta - df[f'{marker_id}_y']  * sin_theta\n",
    "        df_rotated[f'{marker_id}_y'] = df[f'{marker_id}_x'] * sin_theta + df[f'{marker_id}_y']  * cos_theta\n",
    "    return df_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def convert_df_to_cm(df: pd.DataFrame, conversion_factor: float) -> pd.DataFrame:\n",
    "    for marker_id in twoD.utils.get_all_unique_marker_ids(df = df):\n",
    "        df.loc[:, [f'{marker_id}_x', f'{marker_id}_y']] *= conversion_factor\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
