{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utility functions for two dimensional gait analyses\n",
    "\n",
    "> Basic functions used throughout the 2D module and/or that foster the use of this module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp twoD/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from gait_analysis.core import EventBout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def process_all_dlc_tracking_h5s_with_default_settings(in_dir_path: Path, # path to the input directory which contains all DLC tracking data results\n",
    "                                                       week_id: int, # number of weeks post injection\n",
    "                                                       out_dir_path: Path # path to the output directory where all processed results will be saved\n",
    "                                                      ) -> None:\n",
    "    filepaths_dlc_trackings = []\n",
    "    for filepath in in_dir_path.iterdir():\n",
    "        if filepath.name.endswith('.h5'):\n",
    "            if 'filtered' not in filepath.name:\n",
    "                filepaths_dlc_trackings.append(filepath)\n",
    "    for filepath in tqdm(filepaths_dlc_trackings):\n",
    "        recording = Tracked2DRecording(filepath = filepath, week_id = week_id)\n",
    "        if recording.df_successfully_loaded:\n",
    "            recording.preprocess()\n",
    "            if recording.logs['coverage_critical_markers'] >= recording.logs['coverage_threshold']: \n",
    "                recording.run_event_detection()\n",
    "                recording.export_results(out_dir_path = out_dir_path)\n",
    "                recording.inspect_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def filter_dataframe(df: pd.DataFrame, filter_criteria: List[Tuple]) -> pd.DataFrame:\n",
    "    # assert all list have equal lenghts\n",
    "    valid_idxs_per_criterion = []\n",
    "    for column_name, comparison_method, reference_value in filter_criteria:\n",
    "        # assert valid key in comparison methods\n",
    "        # assert column name exists\n",
    "        if comparison_method == 'greater':\n",
    "            valid_idxs_per_criterion.append(df.loc[df[column_name] > reference_value].index.values)\n",
    "        elif comparison_method == 'smaller':\n",
    "            valid_idxs_per_criterion.append(df.loc[df[column_name] < reference_value].index.values)\n",
    "        elif comparison_method == 'equal_to':\n",
    "            valid_idxs_per_criterion.append(df.loc[df[column_name] == reference_value].index.values)\n",
    "        elif comparison_method == 'is_in_list':\n",
    "            valid_idxs_per_criterion.append(df.loc[df[column_name].isin(reference_value)].index.values)\n",
    "        elif comparison_method == 'is_nan':\n",
    "            valid_idxs_per_criterion.append(df.loc[df[column_name].isnull()].index.values)\n",
    "    shared_valid_idxs_across_all_criteria = valid_idxs_per_criterion[0]\n",
    "    if len(valid_idxs_per_criterion) > 1:\n",
    "        for i in range(1, len(valid_idxs_per_criterion)):\n",
    "            shared_valid_idxs_across_all_criteria = np.intersect1d(shared_valid_idxs_across_all_criteria, valid_idxs_per_criterion[i])\n",
    "    df_filtered = df.loc[shared_valid_idxs_across_all_criteria, :].copy()\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions related to preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_max_odd_n_frames_for_time_interval(fps: int, # frames per second of the recording\n",
    "                                           time_interval: 0.5 # desired maximal time interval in seconds; default = 0.5 s\n",
    "                                          ) -> int:\n",
    "    \"\"\"\n",
    "    For the savgol_filter function of scipy - which will be used during preprocessing to smooth the data -\n",
    "    you need an odd integer as the window_length parameter. This function helps to find the maximum odd number\n",
    "    of frames that still fit within a specified time interval at a given fps.\n",
    "    \"\"\"\n",
    "    assert type(fps) == int, '\"fps\" has to be an integer!'\n",
    "    frames_per_time_interval = fps * time_interval\n",
    "    if frames_per_time_interval % 2 == 0:\n",
    "        max_odd_frame_count = frames_per_time_interval - 1\n",
    "    elif frames_per_time_interval == int(frames_per_time_interval):\n",
    "        max_odd_frame_count = frames_per_time_interval\n",
    "    else:\n",
    "        frames_per_time_interval = int(frames_per_time_interval)\n",
    "        if frames_per_time_interval % 2 == 0:\n",
    "            max_odd_frame_count = frames_per_time_interval - 1\n",
    "        else:\n",
    "            max_odd_frame_count = frames_per_time_interval\n",
    "    assert max_odd_frame_count > 0, f'The specified time interval is too short to fit an odd number of frames'\n",
    "    return int(max_odd_frame_count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_preprocessing_relevant_marker_ids(df: pd.DataFrame, # DataFrame with x, y, and likelihood for tracked marker_ids\n",
    "                                          marker_ids_to_exclude: Optional[List[str]]=None # list of marker_ids to exclude; optional default None\n",
    "                                         ) -> List[str]:\n",
    "    all_marker_ids = get_all_unique_marker_ids(df = df)\n",
    "    relevant_marker_ids = all_marker_ids\n",
    "    if marker_ids_to_exclude != None:\n",
    "        for marker_id_to_exclude in marker_ids_to_exclude:\n",
    "            if marker_id_to_exclude in relevant_marker_ids:\n",
    "                relevant_marker_ids.remove(marker_id_to_exclude)\n",
    "    return relevant_marker_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_all_unique_marker_ids(df: pd.DataFrame) -> List[str]:\n",
    "    unique_marker_ids = []\n",
    "    for column_name in df.columns:\n",
    "        marker_id, _ = column_name.split('_')\n",
    "        if marker_id not in unique_marker_ids:\n",
    "            unique_marker_ids.append(marker_id)\n",
    "    return unique_marker_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def smooth_tracked_coords_and_likelihood(df: pd.DataFrame, # DataFrame to smooth\n",
    "                                         window_length: int, # Odd integer (!) of sliding window size in frames to consider for smoothing\n",
    "                                         marker_ids: List[str]=['all'], # List of markers that will be smoothed; optional default ['all'] to smooth all marker_ids\n",
    "                                         polyorder: int=3 # Order of the polynom used for the savgol filter\n",
    "                                        ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Smoothes the DataFrame basically using the implementation from DLC2kinematics:\n",
    "    https://github.com/AdaptiveMotorControlLab/DLC2Kinematics/blob/82e7e60e00e0efb3c51e024c05a5640c91032026/src/dlc2kinematics/preprocess.py#L64\n",
    "    However, with one key change: likelihoods will also be smoothed.\n",
    "    In addition, we will not smooth the columns for the tracked LEDs and the MazeCorners.\n",
    "\n",
    "    Note: window_length has to be an odd integer!\n",
    "    \"\"\"\n",
    "    smoothed_df = df.copy()\n",
    "    column_names = get_column_names(df = smoothed_df,\n",
    "                                    column_identifiers = ['x', 'y', 'likelihood'],\n",
    "                                    marker_ids = marker_ids)\n",
    "    column_idxs_to_smooth = smoothed_df.columns.get_indexer(column_names)\n",
    "    smoothed_df.iloc[:, column_idxs_to_smooth] = savgol_filter(x = smoothed_df.iloc[:, column_idxs_to_smooth],\n",
    "                                                               window_length = window_length,\n",
    "                                                               polyorder = polyorder,\n",
    "                                                               axis = 0)\n",
    "    return smoothed_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_column_names(df: pd.DataFrame, \n",
    "                     column_identifiers: List[str], \n",
    "                     marker_ids: List[str]=['all'],\n",
    "                    ) -> List[str]:\n",
    "    matching_column_names = []\n",
    "    for column_name in df.columns:\n",
    "        marker_id, column_identifier = column_name.split('_')\n",
    "        if marker_ids == ['all']:\n",
    "            if column_identifier in column_identifiers:\n",
    "                matching_column_names.append(column_name)\n",
    "        else:\n",
    "            if (marker_id in marker_ids) and (column_identifier in column_identifiers):\n",
    "                matching_column_names.append(column_name)\n",
    "    return matching_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def interpolate_low_likelihood_intervals(df: pd.DataFrame, \n",
    "                                         marker_ids: List[str], \n",
    "                                         max_interval_length: int,\n",
    "                                         framerate: float,\n",
    "                                        ) -> pd.DataFrame:\n",
    "    interpolated_df = df.copy()\n",
    "    for marker_id in marker_ids:\n",
    "        low_likelihood_interval_border_idxs = get_low_likelihood_interval_border_idxs(likelihood_series = interpolated_df[f'{marker_id}_likelihood'], \n",
    "                                                                                      max_interval_length = max_interval_length, \n",
    "                                                                                      framerate = framerate)\n",
    "        for start_idx, end_idx in low_likelihood_interval_border_idxs:\n",
    "            if (start_idx - 1 >= 0) and (end_idx + 2 < interpolated_df.shape[0]):\n",
    "                interpolated_df[f'{marker_id}_x'][start_idx - 1 : end_idx + 2] = interpolated_df[f'{marker_id}_x'][start_idx - 1 : end_idx + 2].interpolate()\n",
    "                interpolated_df[f'{marker_id}_y'][start_idx - 1 : end_idx + 2] = interpolated_df[f'{marker_id}_y'][start_idx - 1 : end_idx + 2].interpolate()\n",
    "                interpolated_df[f'{marker_id}_likelihood'][start_idx : end_idx + 1] = 0.5\n",
    "    return interpolated_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_low_likelihood_interval_border_idxs(likelihood_series: pd.Series,\n",
    "                                            framerate: float,\n",
    "                                            max_interval_length: int,\n",
    "                                            min_likelihood_threshold: float=0.5\n",
    "                                           ) -> List[Tuple[int, int]]:\n",
    "    all_low_likelihood_idxs = np.where(likelihood_series.values < min_likelihood_threshold)[0]\n",
    "    short_low_likelihood_interval_border_idxs = get_interval_border_idxs(all_matching_idxs = all_low_likelihood_idxs,\n",
    "                                                                               framerate = framerate,\n",
    "                                                                               max_interval_duration = max_interval_length*framerate)\n",
    "    return short_low_likelihood_interval_border_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_interval_border_idxs(all_matching_idxs: np.ndarray,\n",
    "                              framerate: float,\n",
    "                              min_interval_duration: Optional[float]=None, \n",
    "                              max_interval_duration: Optional[float]=None,\n",
    "                             ) -> List[Tuple[int, int]]:\n",
    "    interval_border_idxs = []\n",
    "    if all_matching_idxs.shape[0] >= 1:\n",
    "        step_idxs = np.where(np.diff(all_matching_idxs) > 1)[0]\n",
    "        step_end_idxs = np.concatenate([step_idxs, np.array([all_matching_idxs.shape[0] - 1])])\n",
    "        step_start_idxs = np.concatenate([np.array([0]), step_idxs + 1])\n",
    "        interval_start_idxs = all_matching_idxs[step_start_idxs]\n",
    "        interval_end_idxs = all_matching_idxs[step_end_idxs]\n",
    "        for start_idx, end_idx in zip(interval_start_idxs, interval_end_idxs):\n",
    "            interval_frame_count = (end_idx+1) - start_idx\n",
    "            interval_duration = interval_frame_count * framerate          \n",
    "            if (min_interval_duration != None) and (max_interval_duration != None):\n",
    "                append_interval = min_interval_duration <= interval_duration <= max_interval_duration \n",
    "            elif min_interval_duration != None:\n",
    "                append_interval = min_interval_duration <= interval_duration\n",
    "            elif max_interval_duration != None:\n",
    "                append_interval = interval_duration <= max_interval_duration\n",
    "            else:\n",
    "                append_interval = True\n",
    "            if append_interval:\n",
    "                interval_border_idxs.append((start_idx, end_idx))\n",
    "    return interval_border_idxs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def add_new_marker_derived_from_existing_markers(df: pd.DataFrame,\n",
    "                                                 existing_markers: List[str],\n",
    "                                                 new_marker_id: str,\n",
    "                                                 likelihood_threshold: float = 0.5\n",
    "                                                )->None:\n",
    "    df_with_new_marker = df.copy()\n",
    "    for coordinate in ['x', 'y']:\n",
    "        df_with_new_marker[f'{new_marker_id}_{coordinate}'] = (sum([df_with_new_marker[f'{marker_id}_{coordinate}'] for marker_id in existing_markers]))/len(existing_markers)\n",
    "    df_with_new_marker[f'{new_marker_id}_likelihood'] = 0\n",
    "    row_idxs_where_all_likelihoods_exceeded_threshold = get_idxs_where_all_markers_exceed_likelihood(df = df_with_new_marker, \n",
    "                                                                                                       marker_ids = existing_markers, \n",
    "                                                                                                       likelihood_threshold = 0.5)\n",
    "    df_with_new_marker.iloc[row_idxs_where_all_likelihoods_exceeded_threshold, -1] = 1\n",
    "    return df_with_new_marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_idxs_where_all_markers_exceed_likelihood(df: pd.DataFrame,\n",
    "                                                  marker_ids: List[str],\n",
    "                                                  likelihood_threshold: float=0.5\n",
    "                                                 ) -> np.ndarray:\n",
    "    valid_idxs_per_marker_id = []\n",
    "    for marker_id in marker_ids:\n",
    "        valid_idxs_per_marker_id.append(df.loc[df[f'{marker_id}_likelihood'] >= likelihood_threshold].index.values)\n",
    "    shared_valid_idxs_for_all_markers = valid_idxs_per_marker_id[0]\n",
    "    if len(valid_idxs_per_marker_id) > 1:\n",
    "        for i in range(1, len(valid_idxs_per_marker_id)):\n",
    "            shared_valid_idxs_for_all_markers = np.intersect1d(shared_valid_idxs_for_all_markers, valid_idxs_per_marker_id[i])\n",
    "    return shared_valid_idxs_for_all_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def compute_coverage(df: pd.DataFrame,\n",
    "                     critical_marker_ids: List[str],\n",
    "                     likelihood_threshold: float=0.5\n",
    "                    ) -> float:\n",
    "    idxs_where_all_markers_exceed_likelihood_threshold = get_idxs_where_all_markers_exceed_likelihood(df = df, \n",
    "                                                                                                    marker_ids = critical_marker_ids,\n",
    "                                                                                                    likelihood_threshold = likelihood_threshold)\n",
    "    return idxs_where_all_markers_exceed_likelihood_threshold.shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_corner_coords_with_likelihoods(df: pd.DataFrame) -> Dict:\n",
    "    corner_coords_with_likelihood = {}\n",
    "    for corner_marker_id in ['MazeCornerClosedRight', 'MazeCornerClosedLeft', 'MazeCornerOpenRight', 'MazeCornerOpenLeft']:\n",
    "        xy_coords, min_likelihood = get_most_reliable_marker_position_with_likelihood(df = df, marker_id = corner_marker_id)\n",
    "        corner_coords_with_likelihood[corner_marker_id] = {'coords': xy_coords, 'min_likelihood': min_likelihood}\n",
    "    return corner_coords_with_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_most_reliable_marker_position_with_likelihood(df: pd.DataFrame,\n",
    "                                                      marker_id: str,\n",
    "                                                      percentile: float=99.95\n",
    "                                                     ) -> Tuple[np.array, float]:\n",
    "    likelihood_threshold = np.nanpercentile(df[f'{marker_id}_likelihood'].values, percentile)\n",
    "    df_most_reliable_frames = df.loc[df[f'{marker_id}_likelihood'] >= likelihood_threshold].copy()\n",
    "    most_reliable_x, most_reliable_y = df_most_reliable_frames[f'{marker_id}_x'].median(), df_most_reliable_frames[f'{marker_id}_y'].median()\n",
    "    return np.array([most_reliable_x, most_reliable_y]), likelihood_threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_translation_vector(coords_to_become_origin: np.ndarray) -> np.ndarray:\n",
    "    return -coords_to_become_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def evaluate_maze_shape_using_open_corners(corners_and_likelihoods: Dict, tolerance: float) -> Dict:\n",
    "    best_result = {'valid': False, 'mean_error': tolerance + 1, 'open_corner_id': None, 'side_id': None}\n",
    "    all_open_corner_marker_ids = [corner_marker_id for corner_marker_id in corners_and_likelihoods.keys() if 'Open' in corner_marker_id]\n",
    "    for open_corner_marker_id in all_open_corner_marker_ids:\n",
    "        valid_positions = False\n",
    "        side_id = open_corner_marker_id[open_corner_marker_id.find('Open') + 4:]\n",
    "        if side_id == 'Left': opposite_side_id = 'Right'\n",
    "        else: opposite_side_id = 'Left'\n",
    "        closed_corner_opposite_side = f'MazeCornerClosed{opposite_side_id}'\n",
    "        angle_error = compute_angle_error(a = corners_and_likelihoods[f'MazeCornerClosed{opposite_side_id}']['coords'],\n",
    "                                          b = corners_and_likelihoods[f'MazeCornerClosed{side_id}']['coords'],\n",
    "                                          c = corners_and_likelihoods[open_corner_marker_id]['coords'])\n",
    "        distance_ratio_error = compute_distance_ratio_error(corners_and_likelihoods = corners_and_likelihoods,\n",
    "                                                            open_corner_marker_id = open_corner_marker_id,\n",
    "                                                            side_id = side_id)\n",
    "        if (angle_error <= tolerance) & (distance_ratio_error <= tolerance):\n",
    "            valid_positions = True\n",
    "        mean_error = (angle_error + distance_ratio_error) / 2\n",
    "        if mean_error < best_result['mean_error']:\n",
    "            best_result['valid'] = valid_positions\n",
    "            best_result['mean_error'] = mean_error\n",
    "            best_result['open_corner_id'] = open_corner_marker_id\n",
    "            best_result['side_id'] = side_id\n",
    "    return best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def compute_error_proportion(query_value: float, target_value: float) -> float:\n",
    "    return abs(query_value - target_value) / target_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def compute_angle_error(a: np.ndarray, b: np.ndarray, c: np.ndarray) -> float:\n",
    "    # b is point at the joint that connects the other two\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.degrees(np.arccos(cosine_angle))\n",
    "    return compute_error_proportion(query_value = angle, target_value = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def compute_distance_ratio_error(corners_and_likelihoods: Dict, open_corner_marker_id: str, side_id: str) -> float:\n",
    "    maze_width = et_distance_between_two_points(corners_and_likelihoods['MazeCornerClosedLeft']['coords'],\n",
    "                                                       corners_and_likelihoods['MazeCornerClosedRight']['coords'])\n",
    "    maze_length = get_distance_between_two_points(corners_and_likelihoods[f'MazeCornerClosed{side_id}']['coords'],\n",
    "                                                       corners_and_likelihoods[open_corner_marker_id]['coords'])\n",
    "    distance_ratio = maze_length/maze_width\n",
    "    return compute_error_proportion(query_value = distance_ratio, target_value = 50/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_distance_between_two_points(coords_point_a: np.ndarray, coords_point_b: np.ndarray) -> float:\n",
    "    return ((coords_point_a[0] - coords_point_b[0])**2 + (coords_point_a[1] - coords_point_b[1])**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_conversion_factor_px_to_cm(coords_point_a: np.ndarray, coords_point_b: np.ndarray, distance_in_cm: float) -> float:\n",
    "    distance = get_distance_between_two_points(coords_point_a, coords_point_b)\n",
    "    return distance_in_cm / distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_rotation_angle_with_open_corner(corners: Dict, side_id: str, translation_vector: np.ndarray, conversion_factor: float) -> float:\n",
    "    \"\"\"\n",
    "    Function, that calculates the rotation angle of the maze considering the best matching open corner\n",
    "    and the corresponding closed corner on the same side.\n",
    "\n",
    "    Returns:\n",
    "        float: angle in radians\n",
    "    \"\"\"\n",
    "    if side_id == 'Left':\n",
    "        side_specific_y = 0\n",
    "    else:\n",
    "        side_specific_y = 4\n",
    "    translated_closed_corner = corners[f'MazeCornerClosed{side_id}']['coords'] + translation_vector\n",
    "    translated_open_corner = corners[f'MazeCornerOpen{side_id}']['coords'] + translation_vector\n",
    "    target_rotated_open_corner = np.asarray([50 / conversion_factor, side_specific_y / conversion_factor])\n",
    "    length_a = get_distance_between_two_points(translated_open_corner, target_rotated_open_corner) * conversion_factor\n",
    "    length_b = get_distance_between_two_points(translated_open_corner, translated_closed_corner) * conversion_factor\n",
    "    length_c = 50\n",
    "    angle = math.acos((length_b**2 + length_c**2 - length_a**2) / (2 * length_b * length_c))\n",
    "    return angle  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_rotation_angle_with_closed_corners_only(corners: Dict, translation_vector: np.ndarray, conversion_factor: float) -> float:\n",
    "    translated_closed_left = corners['MazeCornerClosedLeft']['coords'] + translation_vector\n",
    "    translated_closed_right = corners['MazeCornerClosedRight']['coords'] + translation_vector\n",
    "    target_rotated_closed_right = np.asarray([0, 4 / conversion_factor])\n",
    "\n",
    "    length_a = get_distance_between_two_points(translated_closed_right, target_rotated_closed_right) * conversion_factor\n",
    "    length_b = get_distance_between_two_points(translated_closed_left, translated_closed_right) * conversion_factor\n",
    "    length_c = 4\n",
    "    angle = math.acos((length_b**2 + length_c**2 - length_a**2) / (2 * length_b * length_c))\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def normalize_df(df: pd.DataFrame, normalization_parameters)->None:\n",
    "    unadjusted_df = df.copy()\n",
    "    translated_df = translate_df(df = unadjusted_df, translation_vector = normalization_parameters['translation_vector'])\n",
    "    rotated_and_translated_df = rotate_df(df = translated_df, rotation_angle = normalization_parameters['rotation_angle'])\n",
    "    final_df = convert_df_to_cm(df = rotated_and_translated_df, conversion_factor = normalization_parameters['conversion_factor'])\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def translate_df(df: pd.DataFrame, translation_vector: np.array) -> pd.DataFrame:\n",
    "    for marker_id in get_all_unique_marker_ids(df = df):\n",
    "        df.loc[:, [f'{marker_id}_x', f'{marker_id}_y']] += translation_vector\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def rotate_df(df: pd.DataFrame, # DataFrame with 2D coordinates to be rotated\n",
    "              rotation_angle: float # rotation angle in radians\n",
    "             ) -> pd.DataFrame:\n",
    "    df_rotated = df.copy()\n",
    "    cos_theta, sin_theta = math.cos(rotation_angle), math.sin(rotation_angle)\n",
    "    for marker_id in get_all_unique_marker_ids(df = df):\n",
    "        df_rotated[f'{marker_id}_x'] = df[f'{marker_id}_x'] * cos_theta - df[f'{marker_id}_y']  * sin_theta\n",
    "        df_rotated[f'{marker_id}_y'] = df[f'{marker_id}_x'] * sin_theta + df[f'{marker_id}_y']  * cos_theta\n",
    "    return df_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def convert_df_to_cm(df: pd.DataFrame, conversion_factor: float) -> pd.DataFrame:\n",
    "    for marker_id in get_all_unique_marker_ids(df = df):\n",
    "        df.loc[:, [f'{marker_id}_x', f'{marker_id}_y']] *= conversion_factor\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions related to run_event_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_behavior_df(normalized_df: pd.DataFrame, bodyparts_to_include: List[str]) -> pd.DataFrame:\n",
    "    column_names = get_column_names(df = normalized_df, column_identifiers = ['x', 'y', 'likelihood'], marker_ids = bodyparts_to_include)\n",
    "    return normalized_df[column_names].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def add_orientation_to_behavior_df(behavior_df: pd.DataFrame,\n",
    "                                   all_bodyparts: Dict,\n",
    "                                   bodyparts_for_direction_front_to_back: List[str]) -> pd.DataFrame:\n",
    "    assert len(bodyparts_for_direction_front_to_back) ==2, '\"bodyparts_for_direction_front_to_back\" must be a list of exact 2 marker_ids!'\n",
    "    front_marker_id = bodyparts_for_direction_front_to_back[0]\n",
    "    back_marker_id = bodyparts_for_direction_front_to_back[1]\n",
    "    behavior_df.loc[all_bodyparts[front_marker_id].df['x'] > all_bodyparts[back_marker_id].df['x'], 'facing_towards_open_end'] = True\n",
    "    return behavior_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def add_immobility_based_on_several_bodyparts_to_behavior_df(behavior_df: pd.DataFrame,\n",
    "                                                             all_bodyparts: Dict,\n",
    "                                                             bodyparts_critical_for_freezing: List[str]) -> pd.DataFrame:\n",
    "    \n",
    "    # ToDo: Shares some code witht he \"filter_dataframe\" function, can it be reused here?\n",
    "    #       However, here we iterate through several dfs and use the shared indices across \n",
    "    #       These dataframes, so the behavior is different and adaptations would be required.\n",
    "    valid_idxs_per_marker_id = []\n",
    "    for bodypart_id in bodyparts_critical_for_freezing:\n",
    "        tmp_df = all_bodyparts[bodypart_id].df.copy()\n",
    "        valid_idxs_per_marker_id.append(tmp_df.loc[tmp_df['immobility'] == True].index.values)\n",
    "    shared_valid_idxs_for_all_markers = valid_idxs_per_marker_id[0]\n",
    "    if len(valid_idxs_per_marker_id) > 1:\n",
    "        for next_set_of_valid_idxs in valid_idxs_per_marker_id[1:]:\n",
    "            shared_valid_idxs_for_all_markers = np.intersect1d(shared_valid_idxs_for_all_markers, next_set_of_valid_idxs)\n",
    "    behavior_df.loc[shared_valid_idxs_for_all_markers, 'immobility'] = True\n",
    "    return behavior_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_immobility_related_events(behavior_df: pd.DataFrame, fps: float, min_interval_duration: float, event_type: str) -> List[EventBout]:\n",
    "    all_immobility_idxs = np.where(behavior_df['immobility'].values == True)[0]\n",
    "    immobility_interval_border_idxs = get_interval_border_idxs(all_matching_idxs = all_immobility_idxs,\n",
    "                                                              framerate = 1/fps,\n",
    "                                                              min_interval_duration = min_interval_duration)\n",
    "    immobility_related_events = create_event_objects(interval_border_idxs = immobility_interval_border_idxs, fps = fps, event_type = event_type)\n",
    "    return immobility_related_events   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def create_event_objects(interval_border_idxs: List[Tuple[int, int]], fps: int, event_type: str) -> List[EventBout]:\n",
    "    events = []\n",
    "    event_id = 0\n",
    "    for start_idx, end_idx in interval_border_idxs:\n",
    "        single_event = EventBout2D(event_id = event_id, start_idx = start_idx, end_idx = end_idx, fps = fps, event_type = event_type)\n",
    "        events.append(single_event)\n",
    "        event_id += 1\n",
    "    return events  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def add_event_bouts_to_behavior_df(behavior_df: pd.DataFrame, event_type: str, events: List[EventBout]) -> pd.DataFrame:\n",
    "    assert event_type not in list(behavior_df.columns), f'{event_type} was already a column in self.behavior_df!'\n",
    "    behavior_df[event_type] = np.nan\n",
    "    behavior_df[f'{event_type}_id'] = np.nan\n",
    "    behavior_df[f'{event_type}_duration'] = np.nan\n",
    "    if len(events) > 0:\n",
    "        for event_bout in events:\n",
    "            assert event_bout.event_type == event_type, f'Event types didnÂ´t match! Expected {event_type} but found {event_bout.event_type}.'\n",
    "            behavior_df.iloc[event_bout.start_idx : event_bout.end_idx + 1, -3] = True\n",
    "            behavior_df.iloc[event_bout.start_idx : event_bout.end_idx + 1, -2] = event_bout.id\n",
    "            behavior_df.iloc[event_bout.start_idx : event_bout.end_idx + 1, -1] = event_bout.duration\n",
    "    return behavior_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_gait_events(all_bodyparts: Dict, fps: int, gait_min_rolling_speed: float, gait_min_duration: float) -> List[EventBout]:\n",
    "    idxs_with_sufficient_speed = np.where(all_bodyparts['CenterOfGravity'].df['rolling_speed_cm_per_s'].values >= gait_min_rolling_speed)[0]\n",
    "    gait_interval_border_idxs = get_interval_border_idxs(all_matching_idxs = idxs_with_sufficient_speed,\n",
    "                                                        framerate = 1/fps,\n",
    "                                                        min_interval_duration = gait_min_duration)\n",
    "    gait_events = create_event_objects(interval_border_idxs = gait_interval_border_idxs, fps = fps event_type = 'gait_bout')\n",
    "    return gait_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_gait_disruption_events(behavior_df: pd.DataFrame, fps: int, gait_events: List[EventBout], gait_disruption_max_time_to_immobility: float) -> List[EventBout]:\n",
    "    n_frames_max_distance = int(gait_disruption_max_time_to_immobility * fps)\n",
    "    gait_disruption_interval_border_idxs = []\n",
    "    for gait_bout in gait_events:\n",
    "        end_idx = gait_bout.end_idx\n",
    "        unique_immobility_bout_values = behavior_df.loc[end_idx : end_idx + n_frames_max_distance + 1, 'immobility_bout'].unique()\n",
    "        if True in unique_immobility_bout_values:\n",
    "            closest_immobility_bout_id = behavior_df.loc[end_idx : end_idx + n_frames_max_distance + 1, 'immobility_bout_id'].dropna().unique().min()\n",
    "            immobility_interval_border_idxs = get_interval_border_idxs_from_event_type_and_id(behavior_df = behavior_df,\n",
    "                                                                                              event_type = 'immobility_bout',\n",
    "                                                                                              event_id = closest_immobility_bout_id)\n",
    "            gait_disruption_interval_border_idxs.append(immobility_interval_border_idxs)\n",
    "    gait_disruption_events = create_event_objects(interval_border_idxs = gait_disruption_interval_border_idxs, fps = fps event_type = 'gait_disruption_bout')\n",
    "    return gait_disruption_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_interval_border_idxs_from_event_type_and_id(behavior_df: pd.DataFrame, event_type: str, event_id: int) -> Tuple[int, int]:\n",
    "    interval_idxs = behavior_df.loc[behavior_df[f'{event_type}_id'] == event_id].index.values\n",
    "    return interval_idxs[0], interval_idxs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions related to export_results():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def export_immobility_related_bouts(df: pd.DataFrame, event_type: str; framerate: float) -> pd.DataFrame:\n",
    "    results_per_event = {'bout_id': [],\n",
    "                        'duration': [],\n",
    "                        'CenterOfGravity_x_at_bout_start': [],\n",
    "                        'towards_open_at_bout_start': [],\n",
    "                        'distance_covered_cm': [], \n",
    "                        'start_time': [],\n",
    "                        'end_time': []}\n",
    "    results_per_event['bout_id'] = get_all_bout_ids(df = df, event_type = event_type)\n",
    "    if len(results_per_event['bout_id']) >= 1:\n",
    "        results_per_event['duration'] = get_bout_duration_per_bout_id(df = df, event_type = event_type, event_ids = results_per_event['bout_id'])\n",
    "        x_positions_center_of_gravity_at_interval_borders = get_column_values_at_event_borders(df = df,\n",
    "                                                                                                event_type = event_type,\n",
    "                                                                                                event_ids = results_per_event['bout_id'],\n",
    "                                                                                                column_name = 'CenterOfGravity_x')\n",
    "        results_per_event['CenterOfGravity_x_at_bout_start'] = x_positions_center_of_gravity_at_interval_borders[:, 0]\n",
    "        direction_towards_open_at_interval_borders = get_column_values_at_event_borders(df = df,\n",
    "                                                                                        event_type = event_type,\n",
    "                                                                                        event_ids = results_per_event['bout_id'],\n",
    "                                                                                        column_name = 'facing_towards_open_end')\n",
    "        results_per_event['towards_open_at_bout_start'] = direction_towards_open_at_interval_borders[:, 0]\n",
    "        results_per_event['distance_covered_cm'] = get_distance_covered_per_event(df = df, \n",
    "                                                                                   event_type = event_type,\n",
    "                                                                                   event_ids = results_per_event['bout_id'],\n",
    "                                                                                   marker_id = 'CenterOfGravity')\n",
    "        bout_start_and_end_time = get_interval_start_and_end_time_per_event(df = df,\n",
    "                                                                            event_type = event_type,\n",
    "                                                                            event_ids = results_per_event['bout_id'],\n",
    "                                                                            framerate = framerate)\n",
    "        results_per_event['start_time'] = bout_start_and_end_time[:, 0]\n",
    "        results_per_event['end_time'] = bout_start_and_end_time[:, 1]\n",
    "    return pd.DataFrame(data = results_per_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_bout_ids(df: pd.DataFrame, event_type: str) -> np.ndarray:\n",
    "    return df[f'{event_type}_id'].dropna().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bout_duration_per_bout_id(df: pd.DataFrame, event_type: str, event_ids: List[float]) -> List[float]:\n",
    "    durations = []\n",
    "    for event_id in event_ids:\n",
    "        durations.append(df.loc[df[f'{event_type}_id'] == event_id, f'{event_type}_duration'].iloc[0])\n",
    "    return durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_values_at_event_borders(df: pd.DataFrame, event_type: str, event_ids: List[float], column_name: str) -> np.ndarray:\n",
    "    values_at_interval_borders = []\n",
    "    for event_id in event_ids:\n",
    "        start_value = df.loc[df[f'{event_type}_id'] == event_id, column_name].iloc[0]\n",
    "        end_value = df.loc[df[f'{event_type}_id'] == event_id, column_name].iloc[-1]\n",
    "        values_at_interval_borders.append((start_value, end_value))\n",
    "    return np.asarray(values_at_interval_borders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_covered_per_event(df: pd.DataFrame, event_type: str, event_ids: List[float], marker_id: str) -> List[float]:\n",
    "    distances_per_event = []\n",
    "    for event_id in event_ids:\n",
    "        df_tmp = df.loc[df[f'{event_type}_id'] == event_id].copy()\n",
    "        distances_per_event.append(((df_tmp[f'{marker_id}_x'].diff()**2 + df_tmp[f'{marker_id}_y'].diff()**2)**0.5).cumsum().iloc[-1])\n",
    "    return distances_per_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interval_start_and_end_time_per_event(df: pd.DataFrame, event_type: str, event_ids: List[float], framerate: float) -> np.ndarray:\n",
    "    interval_border_idxs = []\n",
    "    for event_id in event_ids:\n",
    "        start_time, end_time = df.loc[df[f'{event_type}_id'] == event_id].index.values[[0, -1]]*framerate\n",
    "        interval_border_idxs.append((start_time, end_time))\n",
    "    return np.asarray(interval_border_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_gait_related_bouts(df: pd.DataFrame, event_type: str, framerate: float) -> pd.DataFrame:\n",
    "    # ToDo: very similar to \"export_immobility_related_bouts\" - can they be combined to one generalized version?\n",
    "    results_per_event = {'bout_id': [],\n",
    "                        'duration': [],\n",
    "                        'CenterOfGravity_x_at_bout_end': [],\n",
    "                        'towards_open_at_bout_end': [],\n",
    "                        'distance_covered_cm': [], \n",
    "                        'start_time': [],\n",
    "                        'end_time': []}\n",
    "    results_per_event['bout_id'] = get_all_bout_ids(df = df, event_type = event_type)\n",
    "    if len(results_per_event['bout_id']) >= 1:\n",
    "        results_per_event['duration'] = get_bout_duration_per_bout_id(df = df, event_type = event_type, event_ids = results_per_event['bout_id'])\n",
    "        x_positions_center_of_gravity_at_interval_borders = get_column_values_at_event_borders(df = df,\n",
    "                                                                                                event_type = event_type,\n",
    "                                                                                                event_ids = results_per_event['bout_id'],\n",
    "                                                                                                column_name = 'CenterOfGravity_x')\n",
    "        results_per_event['CenterOfGravity_x_at_bout_end'] = x_positions_center_of_gravity_at_interval_borders[:, 1]\n",
    "        direction_towards_open_at_interval_borders = get_column_values_at_event_borders(df = df,\n",
    "                                                                                        event_type = event_type,\n",
    "                                                                                        event_ids = results_per_event['bout_id'],\n",
    "                                                                                        column_name = 'facing_towards_open_end')\n",
    "        results_per_event['towards_open_at_bout_end'] = direction_towards_open_at_interval_borders[:, 1]\n",
    "        results_per_event['distance_covered_cm'] = get_distance_covered_per_event(df = df, \n",
    "                                                                                   event_type = event_type,\n",
    "                                                                                   event_ids = results_per_event['bout_id'],\n",
    "                                                                                   marker_id = 'CenterOfGravity')\n",
    "        bout_start_and_end_time = get_interval_start_and_end_time_per_event(df = df, event_type = event_type, event_ids = results_per_event['bout_id'], framerate = framerate)\n",
    "        results_per_event['start_time'] = bout_start_and_end_time[:, 0]\n",
    "        results_per_event['end_time'] = bout_start_and_end_time[:, 1]\n",
    "    return pd.DataFrame(data = results_per_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session_overview_df(dfs_to_export_with_individual_bout_dfs: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    session_overview = {'bout_type': [],\n",
    "                        'total_bouts_count': [],\n",
    "                        'total_duration': [],\n",
    "                        'total_distance_covered': [],\n",
    "                        'mean_duration': [],\n",
    "                        'mean_distance_covered': [],\n",
    "                        'mean_CenterOfGravity_x': []}\n",
    "    for tab_name, df in dfs_to_export_with_individual_bout_dfs.items():\n",
    "        bout_ids_split_depending_on_direction = get_bout_id_splits_depending_on_direction(df = df)\n",
    "        for split_id, relevant_bout_ids in bout_ids_split_depending_on_direction.items():\n",
    "            session_overview = add_results_to_session_overview(session_overview = session_overview, \n",
    "                                                                 df = df, \n",
    "                                                                 event_type = tab_name, \n",
    "                                                                 event_prefix = split_id, \n",
    "                                                                 bout_ids = relevant_bout_ids)\n",
    "    return pd.DataFrame(data = session_overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bout_id_splits_depending_on_direction(df: pd.DataFrame) -> Dict[str, List[float]]:\n",
    "    towards_open_column_name = get_column_name_from_substring(all_columns = list(df.columns), substring = 'towards_open')\n",
    "    bout_ids_split_by_direction = {'all': list(df['bout_id'].unique()),\n",
    "                                   'towards_open': list(df.loc[df[towards_open_column_name] == True, 'bout_id'].unique()),\n",
    "                                   'towards_closed': list(df.loc[df[towards_open_column_name] != True, 'bout_id'].unique())}\n",
    "    return bout_ids_split_by_direction                                                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_name_from_substring(all_columns: List[str], substring: str) -> str:\n",
    "    matching_column_names = [column_name for column_name in all_columns if substring in column_name]\n",
    "    assert len(matching_column_names) == 1, \\\n",
    "            f'There should be exactly one match for {substring} - however, {len(matching_column_names)} were found: [{matching_column_names}].'\n",
    "    return matching_column_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_results_to_session_overview(session_overview: Dict, df: pd.DataFrame, event_type: str, event_prefix: str, bout_ids: List[float]) -> Dict:\n",
    "    session_overview['bout_type'].append(f'{event_prefix}_{event_type}')\n",
    "    if len(bout_ids) > 0:\n",
    "        session_overview['total_bouts_count'].append(len(bout_ids))\n",
    "        session_overview['total_duration'].append(df.loc[df['bout_id'].isin(bout_ids), 'duration'].cumsum().iloc[-1])\n",
    "        session_overview['total_distance_covered'].append(df.loc[df['bout_id'].isin(bout_ids), 'distance_covered_cm'].cumsum().iloc[-1])\n",
    "        session_overview['mean_duration'].append(df.loc[df['bout_id'].isin(bout_ids), 'duration'].mean())\n",
    "        session_overview['mean_distance_covered'].append(df.loc[df['bout_id'].isin(bout_ids), 'distance_covered_cm'].mean())\n",
    "        center_of_gravity_x_column_name = get_column_name_from_substring(all_columns = list(df.columns), substring = 'CenterOfGravity_x')\n",
    "        session_overview['mean_CenterOfGravity_x'].append(df.loc[df['bout_id'].isin(bout_ids), center_of_gravity_x_column_name].mean())\n",
    "    else:\n",
    "        session_overview['total_bouts_count'].append(0)\n",
    "        session_overview['total_duration'].append(0)\n",
    "        session_overview['total_distance_covered'].append(0)\n",
    "        session_overview['mean_duration'].append(np.nan)\n",
    "        session_overview['mean_distance_covered'].append(np.nan)\n",
    "        session_overview['mean_CenterOfGravity_x'].append(np.nan)            \n",
    "    return session_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameter_settings_df(logs: Dict) -> pd.DataFrame:\n",
    "    logged_settings = {'parameter': [], 'specified_value': []}\n",
    "    for parameter, value in logs.items():\n",
    "        logged_settings['parameter'].append(parameter)\n",
    "        logged_settings['specified_value'].append(value)\n",
    "    return pd.DataFrame(data = logged_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_xlsx_file_to_disk(base_output_filepath: Path, dfs_to_export: Dict[str, pd.DataFrame]) -> None:\n",
    "    writer = pd.ExcelWriter(f'{base_output_filepath}.xlsx', engine='xlsxwriter')\n",
    "    for tab_name, df in dfs_to_export.items():\n",
    "        df.to_excel(writer, sheet_name = tab_name)\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
