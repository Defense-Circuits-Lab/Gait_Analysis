{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d7b769e-dd8c-4fe7-9bca-48a570b79509",
   "metadata": {},
   "source": [
    "# Approach:\n",
    "\n",
    "1) Create template of synchro motif, which is fps adjustable\n",
    "2) Extract timeseries of LED-pixel intensity from each video\n",
    "3) Match template to extracted timeseries & find best match\n",
    "4) (Optional?) Check which timeseries of higher fps videos fits best to lower fps videos (i.e. frames 0, 4, 8.. or 1, 5, 9.. or 2, 6, 10..)\n",
    "5) (Optional?) Synchro stable for the entire video?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c63ff8-160c-406b-8eb7-b5f536f258db",
   "metadata": {},
   "source": [
    "# Code development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bebcc4e-e628-4b30-b3fb-b4fc7f8267b7",
   "metadata": {},
   "source": [
    "### 1. Create template of synchro motif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e8d3330-2e9f-43bf-bd41-9a40e9334576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Optional\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c1f27f-8cb9-4e4a-ab2f-81257c95ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesTemplate(ABC):\n",
    "\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def template_attribute_string(self) -> str:\n",
    "        # specifies the attribute name where the template timeseries is saved.\n",
    "        # will be used by the adjust_template_timeseries_to_fps method\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def adjust_template_timeseries_to_fps(self, fps: int) -> List[np.ndarray]:\n",
    "        template_timeseries = getattr(self, self.template_attribute_string)\n",
    "        fps_adjusted_templates = []\n",
    "        framerate = fps/1000\n",
    "        max_frames = int(template_timeseries.shape[0] * framerate)\n",
    "        max_offset = 1000 // fps\n",
    "        for offset_in_ms in range(max_offset):\n",
    "            image_timestamps = np.linspace(0+offset_in_ms, template_timeseries.shape[0]+offset_in_ms, max_frames, dtype='int')\n",
    "            while image_timestamps[-1] >= template_timeseries.shape[0]:\n",
    "                image_timestamps = image_timestamps[:-1]\n",
    "            adjusted_template = template_timeseries[image_timestamps].copy()\n",
    "            fps_adjusted_templates.append((adjusted_template, offset_in_ms))\n",
    "        return fps_adjusted_templates   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a35617-eab7-4dba-8a8b-8422a9dc26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotifTemplate(TimeseriesTemplate):\n",
    "    \n",
    "    @property\n",
    "    def template_attribute_string(self) -> str:\n",
    "        return 'template_timeseries'\n",
    "    \n",
    "    \n",
    "    def __init__(self, led_on_time_in_ms: int, on_off_period_length_in_ms: int, motif_duration_in_ms: int):\n",
    "        self.led_on_time_in_ms = led_on_time_in_ms\n",
    "        self.on_off_period_length_in_ms = on_off_period_length_in_ms\n",
    "        self.motif_duration_in_ms = motif_duration_in_ms\n",
    "        self.template_timeseries = self._compute_template_timeseries()\n",
    "        \n",
    "    \n",
    "    def _compute_template_timeseries(self) -> np.ndarray:\n",
    "        led_on_off_period = np.zeros((self.on_off_period_length_in_ms), dtype='float')\n",
    "        led_on_off_period[1:self.led_on_time_in_ms+1] = 1\n",
    "        full_repetitions = self.motif_duration_in_ms // self.on_off_period_length_in_ms\n",
    "        remaining_ms = self.motif_duration_in_ms % self.on_off_period_length_in_ms\n",
    "        motif_template = np.concatenate([led_on_off_period]*(full_repetitions+1))\n",
    "        adjusted_end_index = self.on_off_period_length_in_ms*full_repetitions + remaining_ms\n",
    "        return motif_template[:adjusted_end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c72681-d36f-4e86-971e-0f58ffae04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionTemplate(TimeseriesTemplate):\n",
    "    \n",
    "    @property\n",
    "    def template_attribute_string(self) -> str:\n",
    "        return 'whole_session_template'\n",
    "    \n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.motif_templates = []\n",
    "    \n",
    "    \n",
    "    def _compute_template_timeseries(self) -> Optional[np.ndarray]:\n",
    "        if len(self.motif_templates) > 0:\n",
    "            return self._concatenate_motif_templates()\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    \n",
    "    def add_motif_template(self, motif_template: MotifTemplate) -> None:\n",
    "        self.motif_templates.append(motif_template)\n",
    "        self.whole_session_template = self._update_session_template()\n",
    "    \n",
    "    \n",
    "    def _update_session_template(self) -> np.ndarray:\n",
    "        individual_motif_template_timeseries = [elem.template_timeseries for elem in self.motif_templates]\n",
    "        return np.concatenate(individual_motif_template_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad37a01-2142-4834-95f3-d0b3f889e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = MotifTemplate(led_on_time_in_ms=50, on_off_period_length_in_ms=100, motif_duration_in_ms=8_000)\n",
    "template2 = MotifTemplate(led_on_time_in_ms=80, on_off_period_length_in_ms=1000, motif_duration_in_ms=590_000)\n",
    "\n",
    "session_template = SessionTemplate()\n",
    "session_template.add_motif_template(motif_template = template)\n",
    "session_template.add_motif_template(motif_template = template2)\n",
    "\n",
    "session_template.whole_session_template.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b895c5-05a8-46d3-be69-ac124562b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_session_templates = session_template.adjust_template_timeseries_to_fps(fps = 160)\n",
    "adjusted_session_templates[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50e5bcc-b151-480c-9e8a-1e6ebd523c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(adjusted_session_templates[0][0][:1600])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890af69d-b06a-4afb-a5fe-9151adcc0df8",
   "metadata": {},
   "source": [
    "### 2. Extract timeseries of LED-pixel intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f5ade-907c-498e-a4a3-bb87765834a0",
   "metadata": {},
   "source": [
    "cropping_indices:\n",
    "\n",
    "- bottom (10/08):   468:475, 523:531  -->\n",
    "- bottom (11/08):   505:515, 570:590  --> 509, 581\n",
    "- bottom_b (11/08 - after bump in synchro): 530:540, 555:565 --> 536, 561\n",
    "- top (10 & 11/08): 245:255, 429:437  --> 249, 433\n",
    "- side1 (11/08):    295:305, 555:565  --> 299, 560\n",
    "- side2 (11/08):    287:300, 300:310  --> 293, 304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa5de8bf-f8ef-4dfc-9370-6e1b1c89b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio as iio\n",
    "import ffmpeg\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/ds/GitHub_repos/rapidAligner/')\n",
    "\n",
    "import rapidAligner as ra\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7874f023-ea29-4afd-9ba0-e9d600a0ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio as iio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a17b6a35-f814-464e-b20e-25e5a037cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_video = '/mnt/c/Users/dsege/Downloads/OneDrive_2022-08-18/Setup_Test_220818/calibration_bottom_nocrop.mp4'\n",
    "for i, frame in enumerate(iio.v3.imiter(filepath_video)):\n",
    "    if i > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8dc936-803a-4f46-87a0-2c60a6daa728",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_crop = (\n",
    "    \n",
    "    503, 670\n",
    "    \n",
    "bottom_nocrop = 823, 834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5ecf0ed-9b74-41db-b27e-53f6bf63dc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff1ccc98d00>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(frame[818:829, 830:840])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d61996-f09b-48ed-ab06-19d7d3a09793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda71f0-5a2d-4c05-a3d1-458ab6b6c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame[287:300, 300:310])\n",
    "plt.scatter(4, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257000ad-59f2-47f2-ae92-3c77c20bde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleCamRawCalibrationData:\n",
    "    \n",
    "    def __init__(self, filepath_video: Path, filepath_tracking: Path, led_marker_id: str, box_rows: int=5, box_cols: int=5) -> None:\n",
    "        self.filepath_video = filepath_video\n",
    "        self.filepath_tracking = filepath_tracking\n",
    "        self.led_marker_id = led_marker_id\n",
    "        self.fps = self._load_fps_from_video_metadata()\n",
    "        self.led_center_pixel_coords = self._get_led_center_pixel_coords()\n",
    "        self.led_timeseries = self._extract_led_pixel_intensities(box_rows = box_rows, box_cols = box_cols)\n",
    "        \n",
    "        \n",
    "    def _load_fps_from_video_metadata(self) -> int:\n",
    "        fps = iio.v3.immeta(filepath_video)['fps']\n",
    "        if fps % 1 != 0:\n",
    "            print(f'Warning! The fps of the video is not an integer! -> fps: {fps}')\n",
    "        return int(fps)\n",
    "        \n",
    "    \n",
    "    def _get_led_center_pixel_coords(self) -> Tuple[int, int]:\n",
    "        # load dlc output file\n",
    "        # go over all predicted led marker positions\n",
    "        # calculate median position from those with highest prediction probabilities\n",
    "        # and confirm that they donÂ´t include large shifts (e.g. very low z-scores only)\n",
    "        # return format has to be (row_index, column_index)\n",
    "        # for now, this will solely return the manually determined coords:\n",
    "        coords = {'top': (249, 433),\n",
    "                  'bottom': (509, 581),\n",
    "                  'bottom_b': (536, 561),\n",
    "                  'side1': (299, 560),\n",
    "                  'side2': (293, 304)}\n",
    "        return coords[self.led_marker_id]\n",
    "    \n",
    "    \n",
    "    def _extract_led_pixel_intensities(self, box_rows: int, box_cols: int) ->np.ndarray:\n",
    "        box_row_indices = self._get_start_end_indices_from_center_coord_and_length(center_px = self.led_center_pixel_coords[0],\n",
    "                                                                                       length = box_rows)\n",
    "        box_col_indices = self._get_start_end_indices_from_center_coord_and_length(center_px = self.led_center_pixel_coords[1],\n",
    "                                                                                       length = box_cols)\n",
    "        mean_pixel_intensities = []\n",
    "        for frame in iio.v3.imiter(self.filepath_video):\n",
    "            box_mean_intensity = frame[box_row_indices[0]:box_row_indices[1], \n",
    "                                       box_col_indices[0]:box_col_indices[1]].mean()\n",
    "            mean_pixel_intensities.append(box_mean_intensity)\n",
    "        return np.asarray(mean_pixel_intensities)\n",
    "                        \n",
    "                        \n",
    "    def _get_start_end_indices_from_center_coord_and_length(self, center_px: int, length: int) -> Tuple[int, int]:\n",
    "        start_index = center_px - (length // 2)\n",
    "        end_index = center_px + (length - (length // 2))\n",
    "        return start_index, end_index\n",
    "    \n",
    "    \n",
    "    def find_best_match_of_motif_template(self, motif_template: MotifTemplate, start_time: int=0, end_time: int=-1, plot_result: bool=True) -> Tuple[int, int]:\n",
    "        # ToDo: optional arguments to specify interval where the motif has to be found?\n",
    "        adjusted_motif_timeseries = motif_template.adjust_template_timeseries_to_fps(fps = self.fps)\n",
    "        start_frame_idx = self._get_frame_index_clostest_to_time(time = start_time)\n",
    "        end_frame_idx = self._get_frame_index_clostest_to_time(time = end_time)\n",
    "        best_match_offset, best_match_start_idx = self._get_offset_and_start_index_of_best_match(adjusted_templates = adjusted_motif_timeseries,\n",
    "                                                                                                 start_frame_idx = start_frame_idx, \n",
    "                                                                                                 end_frame_idx = end_frame_idx)\n",
    "        if plot_result:\n",
    "            self._plot_best_alignment_result(template = adjusted_motif_timeseries[best_match_offset][0], start_idx = best_match_start_idx)\n",
    "        return best_match_offset, best_match_start_idx\n",
    "    \n",
    "    \n",
    "    #ToDo: find_best_match_of_session_template()\n",
    "    \n",
    "    \n",
    "    def _get_frame_index_clostest_to_time(self, time: int) -> int:\n",
    "        time_error_message = (f'The specified time: {time} is invalid! Both times have to be an integer '\n",
    "                              'larger than -1, where -1 represents the very last timestamp in the video '\n",
    "                              'and every other integer (e.g.: 1000) the time in ms. Please be aware, that '\n",
    "                              '\"start_time\" has to be larger than \"end_time\" (with end_time == -1 as only '\n",
    "                              'exception) and must be smaller or equal to the total video recording time.')\n",
    "        if time == -1:\n",
    "            return -1\n",
    "        else:\n",
    "            if time < 0:\n",
    "                raise ValueError(time_error_message)\n",
    "            framerate = 1000 / self.fps\n",
    "            closest_frame_idx = round(time / framerate)\n",
    "            if closest_frame_idx >= self.led_timeseries.shape[0]:\n",
    "                raise ValueError(time_error_message)\n",
    "            return closest_frame_idx\n",
    "\n",
    "        \n",
    "    def _get_offset_and_start_index_of_best_match(self, adjusted_templates: List[Tuple[np.ndarray, int]], start_frame_idx: int, end_frame_idx: int) -> Tuple[int, int]:\n",
    "        lowest_sum_of_squared_error_per_template = []\n",
    "        for template_timeseries, _ in adjusted_templates:\n",
    "            alignment_results = self._run_rapid_aligner(query = template_timeseries, subject = self.led_timeseries[start_frame_idx:end_frame_idx])\n",
    "            alignment_results_as_np_array = alignment_results.get()\n",
    "            lowest_sum_of_squared_error_per_template.append(alignment_results_as_np_array.min())\n",
    "        lowest_sum_of_squared_error_per_template = np.asarray(lowest_sum_of_squared_error_per_template)\n",
    "        best_matching_template_index = int(lowest_sum_of_squared_error_per_template.argmin())\n",
    "        best_alignment_results = self._run_rapid_aligner(query=adjusted_templates[best_matching_template_index][0], subject=self.led_timeseries[start_frame_idx:end_frame_idx])\n",
    "        start_index = int(best_alignment_results.argmin())\n",
    "        return best_matching_template_index, start_index\n",
    "\n",
    "\n",
    "    def _run_rapid_aligner(self, query: np.ndarray, subject: np.ndarray) -> np.ndarray:\n",
    "        subject_timeseries_gpu = cp.asarray(subject)\n",
    "        query_timeseries_gpu = cp.asarray(query)\n",
    "        # run on (global) min-max normalized timeseries, assuming that\n",
    "        # led flashes have more or less the max values? board interference?\n",
    "        return ra.ED.zdist(query_timeseries_gpu, subject_timeseries_gpu, mode=\"fft\")\n",
    "    \n",
    "    \n",
    "    def _plot_best_alignment_result(self, template: np.ndarray, start_idx: int) -> None:\n",
    "        end_idx = start_idx + template.shape[0]\n",
    "        fig = plt.figure(figsize=(15, 10), facecolor='white')\n",
    "        gs = fig.add_gridspec(2, 1)\n",
    "        ax_raw = fig.add_subplot(gs[0,0])\n",
    "        ax_raw.plot(self.led_timeseries[start_idx:end_idx])\n",
    "        ax_raw.plot(template)\n",
    "        ax_zscored = fig.add_subplot(gs[1,0])\n",
    "        ax_zscored.plot(self._zscore(array = self.led_timeseries[start_idx:end_idx]))\n",
    "        ax_zscored.plot(self._zscore(array = template))\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def _zscore(self, array: np.ndarray) -> np.ndarray:\n",
    "        return (array-np.mean(array))/np.std(array, ddof=0)\n",
    "    \n",
    "    \n",
    "    def write_synchronized_and_fps_adjusted_calibration_video(self, start_frame_idx: int, offset: int, target_fps: int) -> None:\n",
    "        original_ms_per_frame = self._get_ms_interval_per_frame(fps = self.fps)\n",
    "        led_timeseries_synchronized_to_motif_start = self._crop_led_timeseries_to_motif_start(start_frame_idx = start_frame_idx, offset = offset)\n",
    "        offset_adjusted_timestamps_synchronized_led_timeseries = self._adjust_timestamps_for_offset(offset = offset, \n",
    "                                                                                                    synchronized_timeseries = led_timeseries_synchronized_to_motif_start)\n",
    "        target_fps_adjusted_timestamps = self._get_timestamps_of_target_fps(target_fps = target_fps, \n",
    "                                                                            synchronized_timeseries = led_timeseries_synchronized_to_motif_start)\n",
    "        idxs_of_frames_to_sample = self._find_frame_idxs_closest_to_target_timestamps(target_timestamps = target_fps_adjusted_timestamps, \n",
    "                                                                                      original_timestamps = offset_adjusted_timestamps_synchronized_led_timeseries)\n",
    "        self.synchronized_and_fps_adjusted_led_timeseries = led_timeseries_synchronized_to_motif_start[idxs_of_frames_to_sample]\n",
    "        idxs_of_frames_to_sample_adjusted_for_synchronization = self._adjust_frame_idxs_for_synchronization(frame_idxs = idxs_of_frames_to_sample,\n",
    "                                                                                                            start_frame_idx = start_frame_idx,\n",
    "                                                                                                            offset = offset)\n",
    "        frame_idxs_per_part = self._split_into_ram_digestable_parts(idxs_of_frames_to_sample = idxs_of_frames_to_sample_adjusted_for_synchronization, max_frame_count = 3000)\n",
    "        self._initiate_writing_of_individual_video_parts(frame_idxs_per_part = frame_idxs_per_part, target_fps = 30) \n",
    "    \n",
    "    \n",
    "    def _get_ms_interval_per_frame(self, fps: int) -> float:\n",
    "        return 1000 / fps\n",
    "    \n",
    "        \n",
    "    def _crop_led_timeseries_to_motif_start(self, start_frame_idx: int, offset: int) -> np.ndarray:\n",
    "        offset_adjusted_start_frame_idx = start_frame_idx + self._get_n_frames_to_adjust_for_offset(offset = offset)\n",
    "        return self.led_timeseries[offset_adjusted_start_frame_idx:].copy()\n",
    "                                                                                                            \n",
    "        \n",
    "    def _get_n_frames_to_adjust_for_offset(self, offset: int) -> int:\n",
    "        return round(offset/self.fps)\n",
    "    \n",
    "    \n",
    "    def _adjust_timestamps_for_offset(self, offset: int, synchronized_timeseries: np.ndarray) -> np.ndarray:\n",
    "        original_ms_per_frame = self._get_ms_interval_per_frame(fps = self.fps)\n",
    "        n_frames_to_adjust_for_offset = self._get_n_frames_to_adjust_for_offset(offset = offset)\n",
    "        updated_offset_after_frame_adjustment = offset - n_frames_to_adjust_for_offset*original_ms_per_frame\n",
    "        offset_adjusted_timestamps_led_series = np.arange((synchronized_timeseries.shape[0])*original_ms_per_frame, step=original_ms_per_frame)\n",
    "        return offset_adjusted_timestamps_led_series + updated_offset_after_frame_adjustment\n",
    "        \n",
    "    \n",
    "    def _get_timestamps_of_target_fps(self, target_fps: int, synchronized_timeseries: np.ndarray) -> np.ndarray:\n",
    "        target_ms_per_frame = self._get_ms_interval_per_frame(fps = target_fps)\n",
    "        original_ms_per_frame = self._get_ms_interval_per_frame(fps = self.fps)\n",
    "        max_frames = int((synchronized_timeseries.shape[0] * original_ms_per_frame) / target_ms_per_frame)\n",
    "        return np.arange(max_frames*target_ms_per_frame, step=target_ms_per_frame)\n",
    "        \n",
    "        \n",
    "    def _find_frame_idxs_closest_to_target_timestamps(self, target_timestamps: np.ndarray, original_timestamps: np.ndarray) -> List[int]:\n",
    "        frame_indices_closest_to_target_timestamps = []\n",
    "        for timestamp in target_timestamps:\n",
    "            closest_frame_index = self._find_closest_timestamp_index(original_timestamps=original_timestamps, timestamp=timestamp)\n",
    "            frame_indices_closest_to_target_timestamps.append(closest_frame_index)\n",
    "        return frame_indices_closest_to_target_timestamps\n",
    "    \n",
    "\n",
    "    def _find_closest_timestamp_index(self, original_timestamps: np.ndarray, timestamp: float) -> int:\n",
    "        return np.abs(original_timestamps - timestamp).argmin()\n",
    "    \n",
    "    \n",
    "    def _adjust_frame_idxs_for_synchronization(self, frame_idxs: List[int], start_frame_idx: int, offset: int) -> List[int]:\n",
    "        offset_adjusted_start_frame_idx = start_frame_idx + self._get_n_frames_to_adjust_for_offset(offset = offset)\n",
    "        frame_idxs_array = np.asarray(frame_idxs)\n",
    "        offset_adjusted_frame_idxs = frame_idxs_array + offset_adjusted_start_frame_idx\n",
    "        return list(offset_adjusted_frame_idxs)\n",
    "        \n",
    "        \n",
    "    def _split_into_ram_digestable_parts(self, idxs_of_frames_to_sample: List[int], max_frame_count: int) -> List[List[int]]:\n",
    "        frame_idxs_per_part = []\n",
    "        while len(idxs_of_frames_to_sample) > 3000:\n",
    "            frame_idxs_per_part.append(idxs_of_frames_to_sample[:3000])\n",
    "            idxs_of_frames_to_sample = idxs_of_frames_to_sample[3000:]\n",
    "        frame_idxs_per_part.append(idxs_of_frames_to_sample)\n",
    "        return frame_idxs_per_part\n",
    "    \n",
    "    \n",
    "    def _initiate_writing_of_individual_video_parts(self, frame_idxs_per_part: List[List[int]], target_fps: int) -> None:\n",
    "        for idx, idxs_of_frames_to_sample in enumerate(frame_idxs_per_part):\n",
    "            part_id = str(idx).zfill(3)\n",
    "            self._write_video_to_disk(idxs_of_frames_to_sample = idxs_of_frames_to_sample, target_fps = target_fps, part_id = part_id)\n",
    "        \n",
    "        \n",
    "    def _write_video_to_disk(self, idxs_of_frames_to_sample: List[int], target_fps: int, part_id: Optional[int]=None) -> None:\n",
    "        selected_frames = []\n",
    "        print('load original video')\n",
    "        for i, frame in enumerate(iio.v3.imiter(filepath_video)):\n",
    "            if i in idxs_of_frames_to_sample:\n",
    "                selected_frames.append(frame)\n",
    "        video_array = np.asarray(selected_frames)\n",
    "        if part_id == None:\n",
    "            filepath_out = self.filepath_video.parent.joinpath(f'{self.led_marker_id}_cam_synchronized_for_calibration.mp4')\n",
    "        else:\n",
    "            filepath_out = self.filepath_video.parent.joinpath(f'{self.led_marker_id}_cam_synchronized_for_calibration_part_{part_id}.mp4')\n",
    "        print('writing video to disk')\n",
    "        iio.mimwrite(filepath_out, video_array, fps=target_fps)\n",
    "        print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d1d1fb-5e6e-407e-95ae-8851fa48787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_video = Path('/mnt/c/Users/dsege/Downloads/194_F8-15_220811_OTE/194_F8-15_220811_OTE/194_F8-15_220811_OTE_bottom.mp4')\n",
    "cam_raw_calib_data = SingleCamRawCalibrationData(filepath_video = filepath_video,\n",
    "                                                 filepath_tracking = 'n.a.',\n",
    "                                                 led_marker_id = 'bottom_b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999cc021-2b57-4a8e-ab68-43aca33e7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cam_raw_calib_data.led_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff63bb-7698-429a-8cf0-e077cc8a2486",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cam_raw_calib_data.fps == 160:\n",
    "    cam_raw_calib_data.fps = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c10d1b-b653-4a52-abe0-c1f44a985180",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_match_offset, best_match_start_idx = cam_raw_calib_data.find_best_match_of_motif_template(motif_template=template, end_time = 20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a957d-ceca-4728-bf55-7ecd8aff1da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_match_start_idx / cam_raw_calib_data.fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f427d813-2418-4261-bc5f-3a5c78d9e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_raw_calib_data.write_synchronized_and_fps_adjusted_calibration_video(start_frame_idx = best_match_start_idx, offset = best_match_offset, target_fps = 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73a1899-daa9-4aa7-b3b7-5a8775b92701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08e5a7f-d9cd-4839-a043-e29cd45fcb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_raw_calib_data.synchronized_and_fps_adjusted_led_timeseries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7711a180-160e-43a8-be87-7b54f9bd3d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94858f-f94f-4daf-8558-13b354a23fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4ed6c-5e42-4199-bfc5-bfbc3b651c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "synchronized_led_timeseries[cam_raw_calib_data.led_marker_id] = cam_raw_calib_data.synchronized_and_fps_adjusted_led_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05a896-9b85-4d03-9eb8-50960a59bde1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1c0c7a-f1d0-4302-934a-523084ab26a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir_path = Path('/mnt/c/Users/dsege/Downloads/OneDrive_2022-08-11/Calibration_220811/Calibration_220811_OTE_CalibrationSide2_7X5.mp4')\n",
    "coords = {'top': (249, 433),\n",
    "          'bottom': (509, 581),\n",
    "          'side1': (299, 560),\n",
    "          'side2': (293, 304)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaff2e4-c68b-41e1-a302-ccc69473d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_video_filenames = {'top': 'Calibration_220811_OTE_CalibrationTop_7X5003.AVI',\n",
    "                       'bottom': 'Calibration_220811_OTE_CalibrationBottom_7X5.mp4',\n",
    "                       'side1': 'Calibration_220811_OTE_CalibrationSide1_7X5.mp4',\n",
    "                       'side2': 'Calibration_220811_OTE_CalibrationSide2_7X5.mp4'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfecf0f-af7c-4133-bbf4-a321b531ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_original_led_traces = []\n",
    "for camera_id in coords.keys():\n",
    "    video_filename = org_video_filenames[camera_id]\n",
    "    filepath_video = video_dir_path.joinpath(video_filename)\n",
    "    led_trace = load_led_trace(led_center_pixel_coords = coords[camera_id], filepath_video = filepath_video)\n",
    "    extracted_original_led_traces.append(led_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb2fa8-95ed-4c2a-912e-398dbd99fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_led_traces = []\n",
    "for camera_id in coords.keys():\n",
    "    video_filename = f'{camera_id}_cam_synchronized_for_calibration.mp4'\n",
    "    filepath_video = video_dir_path.joinpath(video_filename)\n",
    "    led_trace = load_led_trace(led_center_pixel_coords = coords[camera_id], filepath_video = filepath_video)\n",
    "    extracted_led_traces.append(led_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc8a13-ad7d-4d2b-a599-18be072c029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_led_trace(led_center_pixel_coords: Tuple[int, int], filepath_video: Path) -> np.ndarray:\n",
    "    box_row_indices = _get_start_end_indices_from_center_coord_and_length(center_px = led_center_pixel_coords[0],\n",
    "                                                                                   length = 4)\n",
    "    box_col_indices = _get_start_end_indices_from_center_coord_and_length(center_px = led_center_pixel_coords[1],\n",
    "                                                                                   length = 4)\n",
    "    mean_pixel_intensities = []\n",
    "    for frame in iio.v3.imiter(filepath_video):\n",
    "        box_mean_intensity = frame[box_row_indices[0]:box_row_indices[1], \n",
    "                                   box_col_indices[0]:box_col_indices[1]].mean()\n",
    "        mean_pixel_intensities.append(box_mean_intensity)\n",
    "    return np.asarray(mean_pixel_intensities)\n",
    "\n",
    "def _get_start_end_indices_from_center_coord_and_length(center_px: int, length: int) -> Tuple[int, int]:\n",
    "    start_index = center_px - (length // 2)\n",
    "    end_index = center_px + (length - (length // 2))\n",
    "    return start_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a2650-6d23-4fca-bfcb-22f2d9052749",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_led_traces = []\n",
    "for led_trace in extracted_original_led_traces:\n",
    "    if led_trace.shape[0] > 5000:\n",
    "        resampled_led_traces.append(led_trace[resample_idxs])\n",
    "    else:\n",
    "        resampled_led_traces.append(led_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb1ba1-d3d9-4d01-8ac0-92c7f1c4813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 10))\n",
    "for led_trace in resampled_led_traces:\n",
    "    plt.plot(led_trace, alpha=0.4)\n",
    "plt.xlim(0,200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c4709-e7ee-45d3-82d0-67c13e4be525",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 10))\n",
    "for led_trace in extracted_led_traces:\n",
    "    plt.plot(led_trace, alpha=0.4)\n",
    "plt.xlim(0,200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d1bac-53ae-4a69-82fd-a6151bb18f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "synchronized_led_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8a540-baff-4e28-905c-e32a91ed01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 10))\n",
    "for camera_id in synchronized_led_timeseries.keys():\n",
    "    plt.plot(synchronized_led_timeseries[camera_id], label=camera_id, alpha=0.4)\n",
    "plt.xlim(0,60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae68ac20-149b-4593-a1fe-717e28fb4949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8688bc0-291f-4a44-854e-62df1017f055",
   "metadata": {},
   "source": [
    "#### Concatenate individual video parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8538ca60-f5be-49f5-b2d5-c89686ec9115",
   "metadata": {},
   "outputs": [],
   "source": [
    "part1 = ffmpeg.input('/mnt/c/Users/dsege/Downloads/194_F8-15_220811_OTE/194_F8-15_220811_OTE/side2_cam_synchronized_for_calibration_part_000.mp4')\n",
    "part2 = ffmpeg.input('/mnt/c/Users/dsege/Downloads/194_F8-15_220811_OTE/194_F8-15_220811_OTE/side2_cam_synchronized_for_calibration_part_001.mp4')\n",
    "part3 = ffmpeg.input('/mnt/c/Users/dsege/Downloads/194_F8-15_220811_OTE/194_F8-15_220811_OTE/side2_cam_synchronized_for_calibration_part_002.mp4')\n",
    "part4 = ffmpeg.input('/mnt/c/Users/dsege/Downloads/194_F8-15_220811_OTE/194_F8-15_220811_OTE/side2_cam_synchronized_for_calibration_part_003.mp4')\n",
    "part5 = ffmpeg.input('/mnt/c/Users/dsege/Downloads/194_F8-15_220811_OTE/194_F8-15_220811_OTE/side2_cam_synchronized_for_calibration_part_004.mp4')\n",
    "part6 = ffmpeg.input('/mnt/c/Users/dsege/Downloads/194_F8-15_220811_OTE/194_F8-15_220811_OTE/side2_cam_synchronized_for_calibration_part_005.mp4')\n",
    "\n",
    "all_parts = [part1, part2, part3, part4, part5, part6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb971c9c-7e86-4974-8ba1-f926a5e26aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames = ffmpeg.concat(all_parts[0], all_parts[1])\n",
    "for part in all_parts[2:]:\n",
    "    all_frames = ffmpeg.concat(all_frames, part)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "783ce1ba-26d4-49e7-85d7-425d687aa6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ffmpeg.output(all_frames, '/mnt/c/Users/dsege/Downloads/194_F8-15_220811_OTE/194_F8-15_220811_OTE/side2_cam_synchronized_for_calibration_all_parts.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99fdae0c-39e9-4937-bffc-3c36711d5ea0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/mnt/c/Users/dsege/Downloads/194_F8-15_220811_OTE/194_F8-15_220811_OTE/side2_cam_synchronized_for_calibration_part_000.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "  Duration: 00:01:40.00, start: 0.000000, bitrate: 681 kb/s\n",
      "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 656x336, 678 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Input #1, mov,mp4,m4a,3gp,3g2,mj2, from '/mnt/c/Users/dsege/Downloads/194_F8-15_220811_OTE/194_F8-15_220811_OTE/side2_cam_synchronized_for_calibration_part_001.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "  Duration: 00:01:40.00, start: 0.000000, bitrate: 678 kb/s\n",
      "    Stream #1:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 656x336, 675 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Input #2, mov,mp4,m4a,3gp,3g2,mj2, from '/mnt/c/Users/dsege/Downloads/194_F8-15_220811_OTE/194_F8-15_220811_OTE/side2_cam_synchronized_for_calibration_part_002.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "  Duration: 00:01:40.00, start: 0.000000, bitrate: 671 kb/s\n",
      "    Stream #2:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 656x336, 668 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Input #3, mov,mp4,m4a,3gp,3g2,mj2, from '/mnt/c/Users/dsege/Downloads/194_F8-15_220811_OTE/194_F8-15_220811_OTE/side2_cam_synchronized_for_calibration_part_003.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "  Duration: 00:01:40.00, start: 0.000000, bitrate: 685 kb/s\n",
      "    Stream #3:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 656x336, 682 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Input #4, mov,mp4,m4a,3gp,3g2,mj2, from '/mnt/c/Users/dsege/Downloads/194_F8-15_220811_OTE/194_F8-15_220811_OTE/side2_cam_synchronized_for_calibration_part_004.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "  Duration: 00:01:40.00, start: 0.000000, bitrate: 689 kb/s\n",
      "    Stream #4:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 656x336, 686 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Input #5, mov,mp4,m4a,3gp,3g2,mj2, from '/mnt/c/Users/dsege/Downloads/194_F8-15_220811_OTE/194_F8-15_220811_OTE/side2_cam_synchronized_for_calibration_part_005.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "  Duration: 00:01:37.87, start: 0.000000, bitrate: 679 kb/s\n",
      "    Stream #5:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 656x336, 677 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "Stream mapping:\n",
      "  Stream #0:0 (h264) -> concat:in0:v0\n",
      "  Stream #1:0 (h264) -> concat:in1:v0\n",
      "  Stream #2:0 (h264) -> concat:in1:v0\n",
      "  Stream #3:0 (h264) -> concat:in1:v0\n",
      "  Stream #4:0 (h264) -> concat:in1:v0\n",
      "  Stream #5:0 (h264) -> concat:in1:v0\n",
      "  concat -> Stream #0:0 (libx264)\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x55801be37c40] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x55801be37c40] profile High, level 3.0\n",
      "[libx264 @ 0x55801be37c40] 264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=10 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '/mnt/c/Users/dsege/Downloads/194_F8-15_220811_OTE/194_F8-15_220811_OTE/side2_cam_synchronized_for_calibration_all_parts.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 656x336, q=-1--1, 30 fps, 15360 tbn, 30 tbc (default)\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=17936 fps=345 q=-1.0 Lsize=   38333kB time=00:09:57.76 bitrate= 525.3kbits/s speed=11.5x    \n",
      "video:38121kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.554230%\n",
      "[libx264 @ 0x55801be37c40] frame I:72    Avg QP:19.51  size: 17160\n",
      "[libx264 @ 0x55801be37c40] frame P:4565  Avg QP:23.60  size:  4094\n",
      "[libx264 @ 0x55801be37c40] frame B:13299 Avg QP:27.07  size:  1437\n",
      "[libx264 @ 0x55801be37c40] consecutive B-frames:  0.5%  1.6%  1.0% 96.9%\n",
      "[libx264 @ 0x55801be37c40] mb I  I16..4: 14.0% 61.7% 24.3%\n",
      "[libx264 @ 0x55801be37c40] mb P  I16..4:  4.4%  5.9%  0.5%  P16..4: 46.4% 13.2%  6.2%  0.0%  0.0%    skip:23.4%\n",
      "[libx264 @ 0x55801be37c40] mb B  I16..4:  1.6%  1.9%  0.1%  B16..8: 33.0%  3.8%  0.4%  direct: 3.9%  skip:55.4%  L0:51.8% L1:44.8% BI: 3.5%\n",
      "[libx264 @ 0x55801be37c40] 8x8 transform intra:54.0% inter:77.4%\n",
      "[libx264 @ 0x55801be37c40] coded y,uvDC,uvAC intra: 25.8% 81.3% 29.4% inter: 13.3% 28.5% 2.6%\n",
      "[libx264 @ 0x55801be37c40] i16 v,h,dc,p: 33% 19% 12% 36%\n",
      "[libx264 @ 0x55801be37c40] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 31% 18% 29%  4%  3%  4%  4%  3%  4%\n",
      "[libx264 @ 0x55801be37c40] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 17% 30% 23%  5%  5%  4%  7%  3%  6%\n",
      "[libx264 @ 0x55801be37c40] i8c dc,h,v,p: 45% 17% 25% 12%\n",
      "[libx264 @ 0x55801be37c40] Weighted P-Frames: Y:1.9% UV:1.9%\n",
      "[libx264 @ 0x55801be37c40] ref P L0: 45.0%  9.5% 33.0% 12.3%  0.3%\n",
      "[libx264 @ 0x55801be37c40] ref B L0: 68.8% 18.5% 12.6%\n",
      "[libx264 @ 0x55801be37c40] ref B L1: 90.1%  9.9%\n",
      "[libx264 @ 0x55801be37c40] kb/s:522.33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.run(overwrite_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5358e23-07b9-4408-be2d-dee6b6976112",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Match template synchro motif to extracted timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be095b3c-0e0f-45e5-bb94-e0c529b22db4",
   "metadata": {},
   "source": [
    "Well, see cells above :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
